{
  
    
        "post0": {
            "title": "Demo Day: Using synthimpute for data fusion",
            "content": ". Suppose a policy analyst sought to estimate the impact of a policy that changed income tax rates and benefit rules while also adding a progressive wealth tax. The standard approach is to use a microsimulation model, where the rules are programmed as code, and then to run that program over a representative sample of households. Unfortunately, no single US government survey captures all the households characteristics needed to analyze this policy; in particular, the reliable tax and benefit information lies in surveys like the Current Population Survey (CPS), while wealth lies in the Survey of Consumer Finances (SCF). . Assuming the analyst wanted to start with the CPS, they have several options to estimate wealth for households to levy the progressive wealth tax. Two typical approaches include: . Linear regression, predicting wealth from other household characteristics common to the CPS and SCF. | Matching, in which each CPS household is matched with the most similar household in the SCF. | Neither of these approaches, however, aim to estimate the distribution of wealth conditional on other characteristics. Linear regression explicitly estimates the mean prediction, but that could miss the tails of wealth from whom most of the wealth tax revenue will be collected. . Instead, the analyst could apply quantile regression to estimate the distribution of wealth conditional on other characteristics, and then measure the effectiveness of the estimation using quantile loss. . In this Demo Day, I present the concepts of microsimulation, imputation, and quantile loss to motivate the synthimpute Python package I’ve developed with my PolicyEngine colleague Nikhil Woodruff. In an experiment predicting wealth on a holdout set from the SCF, my former colleague Deepak Singh and I found that random forests significantly outperform OLS and matching for quantile regression, and this is the approach applied in synthimpute for both data fusion and data synthesis. The synthimpute API will be familiar to users of scikit-learn and statsmodels, with the difference being that synthimpute’s rf_impute function returns a random value from the predicted distribution; it can also skew the predictions to meet a target total. . We’ve used synthimpute to fuse data for research reports at the UBI Center and to enhance the PolicyEngine web app for UK tax and benefit simulation, and we welcome new users and contributors. Feel free to explore the repository or contact me with questions at max@policyengine.org. . Resources: . synthimpute package on GitHub | Presentation slides | UBI Center report on land value taxation in the UK, using synthimpute to impute land value from the UK Wealth and Assets Survey to the Family Resources Survey | PolicyEngine UK carbon tax example, using synthimpute to impute carbon emissions from the Living Costs and Food Survey to the Family Resources Survey | Notebook comparing random forests to matching and other techniques using a holdout set from the US Survey of Consumer Finances | My blog post on quantile regression for Towards Data Science, which laid the groundwork for synthimpute | .",
            "url": "https://blog.pslmodels.org/demo-day-22-synthimpute",
            "relUrl": "/demo-day-22-synthimpute",
            "date": " • Dec 8, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Demo Day: The OG-Core Platform",
            "content": ". The OG-Core model is a general equilibrium, overlapping generations (OG) model suitable for evaluating fiscal policy. Since the work of Alan Auerbach and Laurence Kotlikoff in the 1980s, this class of model has become a standard in the macroeconomic analysis of tax and spending policy. This is for good reason. OG models are able to capture the impacts of taxes and spending in the short and long run, examine incidence of policy across generations of people (not just short run or steady state analysis of a cross-section of the economy), and capture important economic dynamics (e.g., crowding out effects of deficit-financed policy). . In the PSL Demo Day presentation linked above, I cover the basics of OG-Core: its history, its API, and how country-specific models can use OG-Core as a dependency. In brief, OG-Core provides a general overlapping generations framework, from which parameters can be calibrated to represent particular economies. Think of it this way: an economic model is just a set of parameters plus a system of equations. OG-Core spells out all of the equations to represent an economy with heterogeneous agents, production and government sectors, open economy options, and detailed policy rules. OG-Core also includes default values for all parameters, along with parameter metadata and parameter validation rules. A country specific application is then just a particular parameterization of the general OG-Core model. . As an example of a country-specific application, one can look at the OG-USA model. This model provides a calibration of OG-Core to the United States. The source code in that project allows one to go from raw data sources to the estimation and calibration procedures used to determine parameter values representing the United States, to parameter values in formats suitable for use in OG-Core. Country-specific models like OG-USA include (where available) links to microsimulation models of tax and spending programs to allow detailed microdata of actual and counterfactual policies to inform the net tax-transfer functions used in the OG-Core model. For those interested in building their own country-specific model, the OG-USA project provides a good example to work from. . We encourage you to take a look at OG-Core and related projects. New contributions and applications are always welcome. If you have questions or comments, reach out through the relevant repositories on Github to me, @jdebacker, or Rick Evans, @rickecon. . Resources: . OG-Core documentation | OG-USA documentation package for unit testing in Python | Tax-Calculator documentation | OG-UK repository | OpenFisca-UK repository | Slides from the Demo Day presentation | .",
            "url": "https://blog.pslmodels.org/demo-day-21-og_core",
            "relUrl": "/demo-day-21-og_core",
            "date": " • Nov 1, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Demo Day: How to deploy apps on Compute Studio",
            "content": ". . Compute Studio (C/S) is a platform for publishing and sharing computational models and data visualizations. In this demo day, I show how to publish your own project on C/S using the new automated deployments feature. You can find an in depth guide to publishing on C/S in the developer docs. . C/S supports two types of projects: models and data visualizations. Models are fed some inputs and return a result. Data visualizations are web servers backed by popular open-source libraries like Bokeh, Dash, or Streamlit. Models are good for long-running processes and producing archivable results that can be shared and returned to easily. Data visualizations are good for highly interactive and custom user experiences. . Now that you’ve checked out the developer docs and set up your model or data-viz, you can head over to the C/S publishing page https://compute.studio/new/ to publish your project. Note that this page is still very much under construction and may look different in a few weeks. . . Next, you will be sent to the second stage in the publish flow where you will provide more details on how to connect your project on C/S: . . Clicking “Connect App” will take you to the project home page: . . Go to the “Settings” button in the top-right corner and this will take you to the project dashboard where you can modify everything from the social preview of your project to the amount of compute resources it needs: . . The “Builds” link in the sidebar will take you to the builds dashboard where you can create your first build: . . It’s time to create the first build. You can do so by clicking “New Build”. This will take you to the build status page. While the build is being scheduled, the page will look like this: . . You can click the “Build History” link and it will show that the build has been started: . . The build status page should be updated at this point and will look something like this: . . C/S automated deployments are built on top of Github Actions. Unfortunately, the logs in Github Actions are not available through the Github API until after the workflow is completely finished. The build status dashboard will update as the build progresses and once it’s done, you will have full access to the logs from the build. These will contain outputs from installing your project and the outputs from your project’s tests. . In this case, the build failed. We can inspect the logs to see that an import error caused the failure: . . . I pushed an update to my fork of Tax-Cruncher on Github and restarted the build by clicking “Failure. Start new Build”. The next build succeeded and we can click “Release” to publish the project: . . The builds dashboard now shows the two builds: . . Finally, let’s go run our new model: . . It may take a few seconds for the page to load. This is because the model code and all of its dependencies are being loaded onto the C/S servers for the first time: . . The steps for publishing a data visualization are very similar. The main idea is that you tell C/S what Python file your app lives in and C/S will know how to run it given your data visualization technology choice. .",
            "url": "https://blog.pslmodels.org/cs-auto-deploy",
            "relUrl": "/cs-auto-deploy",
            "date": " • Sep 20, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Demo Day: Unit Testing for Open Source Projects",
            "content": ". Unit testing is the testing of individual units or functions of a software application. This differs from regression testing that focuses on the verification of final outputs. Instead, unit testing tests each smallest testable component of your code. This helps to more easily identify and trace errors in the code. . Writing unit tests is good practice, though not one that’s always followed. The biggest barrier to writing unit tests is that doing so takes time. You might wonder “why am I testing code that runs?” But there are a number benefits to writing unit tests: . It ensures that the code does what you expect it to do | You’ll better understand what your code is doing | You will reduce time tracking down bugs in your code | . Often, writing unit tests will save you time in the longer run because it reduces debugging time and because it forces you to think more about what your code does, which often leads to the development of more efficient code. And for open source projects, or projects with many contributors, writing unit tests is important in reducing the likelihood that errors are introduced into your code. This is why the PSL catalog criteria requires projects to provide at least some level of unit testing. . In the PSL Demo Day video linked above, I illustrate how to implement unit tests in R using the testthat package. There are essentially three steps to this process: . Create a directory to put your testing script in, e.g., a folder called tests | Create one or more scripts that define your tests. Each test is represented as a call of the test_that function and contain an statement that will evaluate as true or false (e.g., you may use the expect_equal function to verify that a function returns expected values given certain inputs). | You will want to use test in the name of these tests scripts as well as something descriptive of what is tested. | . | Create a script that will run your tests. Here you’ll need to import the testthat package and you’ll need to call the script(s) you are testing to load their functions. | Then you’ll use the test_dir function to pass the directory in which the script(s) you created in Step 2 reside. | . | Check out the video to see examples of how each of these steps is executed. I’ve also found this blog post on unit tests with testthat to be helpful. . Unit testing in Python seems to be more developed and straightforward with the excellent pytest package. While pytest offers many options for parameterizing tests, running tests in parallel, and more, the basic steps remain the same as those outlined above: . Create a directory for your test modules (call this folder tests as pytest will look for that name). | Create test modules that define each test Tests are defined much like any other function in Python, the but will involve some assertion statement is triggered upon test failure. | You will want to use test in the name of these tests modules as well as something descriptive of what is tested. | . | You won’t need to create a script to run your tests as with testthat, but you may create a pytest.ini file to customize your tests options. | That’s about it to get started writing unit tests for your code. PSL cataloged projects provide many excellent examples of a variety of unit tests, so search them for examples to build from. In a future Demo Day and blog post, we’ll talk about continuous integration testing to help get even more out of you unit tests. . Resources: . testthat package for unit testing in R | pytest package for unit testing in Python | PSL catalog criteria | Unit tests for the capital-cost-recovery model | .",
            "url": "https://blog.pslmodels.org/demo-day-16-unit_testing",
            "relUrl": "/demo-day-16-unit_testing",
            "date": " • Aug 9, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Demo Day: Constructing Tax Data for the 50 States",
            "content": ". Federal income tax reform impacts can vary dramatically across states. The cap on state and local tax deductions (SALT) is a well-known example, but other policies also have differential effects because important tax-relevant features vary across states such as the income distribution, relative importance of wage, business, and retirement income, and family size and structure. Analyzing how policy impacts vary across states requires data that faithfully represent the characteristics of the 50 states. . This Demo Day described a method and software for constructing state weights for microdata files that (1) come as close as possible to targets for individual states, while (2) ensuring that the state weights for each tax record sum to its national weight. The latter objective ensures that the sum of state impacts for a tax reform equals the national impact. . This project developed state weights for a data file with more than 200,000 microdata records. The weighted data file comes within 0.01% of desired values for more than 95% of approximately 10,000 targets. . The goal of the slides and video was to enable a motivated Python-skilled user of the PSL TaxData and Tax-Calculator projects to reproduce project results: 50-state weights for TaxData’s primary output, the puf.csv microdata file (based primarily on an IRS Public Use File), using early-stage open-source software developed in the project. Thus, the demo is technical and focused on nuts and bolts. . The methods and software can also be used to: . Create geographic-area weights for other microdata files | Apportion state weights to Congressional Districts or counties, if suitable targets can be developed | Create state-specific microdata files suitable for modeling state income taxes | . The main topics covered in the slides and video are: . Creating national and state targets from IRS summary data | Preparing a national microdata file for state weighting | Approaches to constructing geographic weights | Running software that implements the Poisson-modeling approach used in the project | Measures of quality of the results | .",
            "url": "https://blog.pslmodels.org/demo-day-14-constructing-tax-data-for-the-50-states",
            "relUrl": "/demo-day-14-constructing-tax-data-for-the-50-states",
            "date": " • Jul 16, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Demo Day: Using the TaxBrain Python API",
            "content": ". . The TaxBrain project was primarily created to serve as the backend of the Tax-Brain web-application. But at its core, TaxBrain is a Python package that greatly simplifies tax policy analysis. For this PSL Demo-Day, I demonstrated TaxBrain’s capabilities as a standalone package, and how to use it to produce high-level summaries of the revenue impacts of proposed tax policies. The Jupyter Notebook from the presentation can be found here. . TaxBrain’s Python API allows you to run a full analysis of income tax policies in just three lines of code: . from taxbrain import TaxBrain tb = TaxBrain(START_YEAR, END_YEAR, use_cps=True, reform=REFORM_POLICY) tb.run() . Where START_YEAR and END_YEAR are the first and last years, respectively, of the analysis; use_cps is a boolean indicator that you want to use the CPS-based microdata file prepared for use with Tax-Calculator; and REFORM_POLICY is either a JSON file or Python dictionary that specifies a reform suitable for Tax-Calculator. The forthcoming release of TaxBrain will also include a feature that allows you to perform a stacked revenue analysis as well. The inspiration for this feature was presented by Jason DeBacker in a previous demo-day. . Once TaxBrain has been run, there are a number of methods and functions included in the package to create tables and plots to summarize the results. I used the Biden 2020 campaign proposal in the demo and the resulting figures are below. The first is a “volcano plot” that makes it easy to see the magnitude of the change in tax liability individuals across the income distribution face. Each dot represents a tax unit, and the x and y variables can be customized based on the user’s needs. . . The second gives a higher-level look at how taxes change in each income bin. It breaks down what percentage of each income bin faces a tax increase or decrease, and the size of that change. . . The final plot shown in the demo simply shows tax liabilities by year over the budget window. . . The last feature I showed was TaxBrain’s automated reports. TaxBrain uses saved results and an included report template to write a report summarizing the findings of your simulation. The reports include tables and figures similar to what you may find in similar write ups by the Joint Committee on Taxation or Tax Policy Center including a summary of significant changes caused by the reform, and all you need is one line of code: . report(tb, name=&#39;Biden Proposal&#39;, outdir=&#39;biden&#39;, author=&#39;Anderson Frailey&#39;) . The above code will save a PDF copy of the report in a directory called biden along with PNG files for each of the graphs created and the raw Markdown text used for the report, which you can then edit as needed if you would like to add content to the report that is not already included. Screenshots of the default report are included below. . . There are of course downsides to using TaxBrain instead of Tax-Calculator directly. Specifically, it’s more difficult, and sometimes impossible, to perform custom tasks like modeling a feature of the tax code that hasn’t been added to Tax-Calculator yet or advanced work with marginal tax rates. But for day-to-day tax modeling, the TaxBrain Python package can significantly simply any workflow. . Resources: . Tax-Brain GitHub repo | Tax-Brain Documentation | .",
            "url": "https://blog.pslmodels.org/demo-day-13-taxbrain-python-api",
            "relUrl": "/demo-day-13-taxbrain-python-api",
            "date": " • Jun 14, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Demo Day: Keeping Jupyter Book documentation up to date with GH Actions",
            "content": ". Open source projects must maintain clear and up-to-date documentation in order to attract users and contributors. Because of this, PSL sets minimum standards for documentation among cataloged projects in its model criteria. A recent innovation in executable books, Jupyter Book, has provided an excellent format for model documentation and has been widely adopted by PSL projects (see for example OG-USA, Tax-Brain, Tax-Calculator). . Jupyter Book allows one to write documents with executable code and text together, as in Jupyter notebooks. But Jupyter Book pushes this further by allowing documents with multiple sections, better integration of TeX for symbols and equations, BibTex style references and citations, links between sections, and Sphinx integration (for auto-built documentation of model APIs from source code). Importantly for sharing documentation, Jupyter Books can easily be compiled to HTML, PDF, or other formats. Portions of a Jupyter Book that contain executable code can be downloaded as Jupyter Notebooks or opened in Google Colab or binder . The Jupyter Book documentation is excellent and will help you get started creating your “book” (tip: pay close attention to formatting details, including proper whitespace). What I do here is outline how you can easily deploy your documentation to the web and keep it up-to-date with your project. . I start from the assumption that you have the source files to build your Jupyter Book checked into the main branch of your project (these maybe yml, md, rst, ipynb or other files). For version control purposes and to keep your repo trim, you generally don’t want to check the built documentation files to this branch (tip: consider adding the folder these files will go to (e.g., /_build to your .gitignore). When these files are in place and you can successfully build your book locally, it’s time for the first step. . Step 1: Add two GH Actions to your project’s workflow: . An action to check that your documentation files build without an error. I like to run this on each push to a PR. The action won’t hang on warnings, but will fail if your Jupyter Book doesn’t build at all. An example of this action from the OG-USA repo is here: | name: Check that docs build on: [push, pull_request] . jobs: build: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v2 # If you’re using actions/checkout@v2 you must set persist-credentials to false in most cases for the deployment to work correctly. with: persist-credentials: false . - name: Setup Miniconda uses: conda-incubator/setup-miniconda@v2 with: activate-environment: ogusa-dev environment-file: environment.yml python-version: 3.7 auto-activate-base: false - name: Build # Build Jupyter Book shell: bash -l {0} run: | pip install jupyter-book pip install sphinxcontrib-bibtex==1.0.0 pip install -e . cd docs jb build ./book To use this in your repo, you&#39;ll just need to change a few settings such as the name of the environment and perhaps the Python version and path to the book source files. Note that in the above `yml` file `sphinxcontrib-bibtex` is pinned. You maybe able to unpin this, but OG-USA needed this pin for documentation to compile property due to changes in the `jupyter-book` and `sphinxcontrib-bibtex` packages. . An action that builds and deploys the Jupyter Book to GH Pages. The OG-USA project uses the deploy action from James Ives in this action. This is something that you will want to run when PRs are merged into your main branch so that the documentation is kept up-to-date with the project. To modify this action for your repo, you’ll need to change the repo name, the environment name, and potentially the Python version, branch name, and path to the book source files. | Step 2: Once the action in (2) above is run, your compiled Jupyter Book docs will be pushed to a gh-pages branch in your repository (the action will create this branch for you if it doesn’t already exist). At this point, you should be able to see your docs at the url https://GH_org_name.github.io/Repo_name. But it probably won’t look very good until you complete this next step. To have your Jupyter Book render on the web as you see it on your machine, you will want to push and merge an empty file with the name .nojekyll into your repo’s gh-pages branch. . That’s it! With these actions, you’ll be sure that your book continues to compile and a new version will be published to the web with with each merge to your main branch, ensuring that your documentation stays up-to-date. . Some additional tips: . Use Sphinx to document your projects API. By doing so you’ll automate an important part of your project’s documentation – as long as the docstrings are updated when the source code is, the Jupyter Book you are publishing to the web will be kept in sync with no additional work needed. | You can have your gh-pages-hosted documentation point to a custom URL. | Project maintainers should ensure that docs are updated with PRs that are relevant (e.g., if the PR changes an the source code affecting a user interface, then documentation showing example usage should be updated) and help contributors make the necessary changes to the documentation source files. | .",
            "url": "https://blog.pslmodels.org/demo-day-12-jupyter-book-deploy",
            "relUrl": "/demo-day-12-jupyter-book-deploy",
            "date": " • May 17, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Demo Day: Producing stacked revenue estimates with the Tax-Calculator Python API",
            "content": ". It’s often useful to be able to identify the effects of specific provisions individually and not just the overall impact of a proposal with many provisions. Indeed, when revenue estimates of tax law changes are reported (such as this JCT analysis of the American Rescue Plan Act of 2021), they are typically reported on a provision-by-provision basis. Finding the provision-by-provision revenue estimates is cumbersome with the Tax-Brain web application both because it’s hard to iterate over many provisions and because the order matters when stacking estimates, so that one needs to keep this order in mind as parameter values are updated for each additional provision in a full proposal. . In the PSL Demo Day on April 5, 2021, I show how to use the Python API of Tax-Calculator to efficiently produce stacked revenue estimates. In fact, after some initial setup, this can be done with just 12 lines of code (plus a few more to make the output look nice). The Google Colab notebook that illustrates this approach can be found at this link, but here I’ll walk through the four steps that are involved: . Divide up the full proposal into strings of JSON text that contain each provision you want to analyze. My example breaks up the Biden 2020 campaign proposal into seven provisions, but this is illustrative and you can make more or less provisions depending on the detail you would like to see. | Create a dictionary that contains, as its values, the JSON strings. A couple notes on this. First, the dictionary keys should be descriptive of the provisions as they will become the labels for each provision in the final table of revenue estimates we produce. Second, order matters here. You’ll want to be sure the current law baseline is first (the value for this will be an empty dictionary). Then you specify the provisions. The order you specify will likely affect your revenue estimates from a given provision (for instance, expanding/restricting a deduction has a larger revenue effect when rates are higher), but there are not hard and fast rules on the “right” order. Traditionally, rate changes are stacked first and tax expenditures later in the order. | Iterate over this dictionary. With a dictionary of provisions in hand, we can write a “for loop” to iterate over the provision, simulating the Tax-Calculator model at each step. Note that when the Policy class object in Tax-Calculator is modified, it only needs to be told the changes in tax law parameters relative to its current state. In other words, when we are stacking provisions, estimating the incremental effect of each, you can think of the Policy object having a baseline policy that is represented by the current law baseline plus all provisions that have been analyzed before the provision at the current iteration. The Policy class was created in this way so that one can easily represent policy changes, requiring the user to only input the set of parameters that are modified, not every single parameter’s value under the hypothetical policy. But this also makes it parsimonious to stack provisions as we are doing here. Notice that the JSON strings for each provision (created in Step 1) can be specified independent of the stacking order. We only needed to slice the full set of proposals into discrete chunks, we didn’t need to worry about creating specifications of cumulative policy changes. | Format output for presentation. After we’ve run a Tax-Calculator simulation for the current law baseline plus each provision (and each year in the budget window), we’ve got all the output we need. With this output, we can quickly create a table that will nicely present our stacked revenue estimate. One good check to do here is to create totals across all provisions and compare this to the simulated revenue effects of running the full set of proposals in one go. This check helps to ensure that you didn’t make an error in specifying your JSON strings. For example, it’s easy to leave out one or more provisions, especially if there are many. | I hope this provides a helpful template for your own analysis. Note that one can modify this code in several useful ways. For example, within the for-loops, the Behavioral-Responses can be called to produce revenue estimates that take into account behavioral feedback. Or one could store the individual income tax and payroll tax revenue impacts separately (rather than return the combined values as in the example notebook). Additional outputs (even the full set of microdata after each provision is applied) can be stored for even more analysis. . In the future, look for Tax-Brain to add stacked revenue estimates to its capabilities. It’ll still be important for users to carve up their full list of policy changes into sets of provisions as we did in Steps 1 and 2 above, but Tax-Brain will then take care of the rest behind the scenes. . Resources: . Colab Notebook with example | Biden campaign reform file in PSL Examples | .",
            "url": "https://blog.pslmodels.org/demo-day-11-stacked-revenue-estimates",
            "relUrl": "/demo-day-11-stacked-revenue-estimates",
            "date": " • Apr 5, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Demo Day: Stitching together Apps on Compute Studio",
            "content": ". In Demo Day 8, I talked about connecting multiple apps on Compute Studio with PSL Stitch. The source code for PSL stitch can be found in this repository. . Stitch is composed of three components: . A python package that can be run like a normal Python package. | A RESTful API built with FastAPI that is called remotely to create simulations on Compute Studio. | A GUI built with ReactJS that makes calls to the REST API to create and monitor simulations. | . One of the cool things about this app is that it uses ParamTools to read the JSON files under the hood. This means that it can read links to data in other Compute Studio runs, files on GitHub, or just plain JSON. Here are some example parameters: . policy parameters: cs://PSLmodels:Tax-Brain@49779/inputs/adjustment/policy | tax-cruncher parameters: {&quot;sage&quot;: [{&quot;value&quot;: 25}]} | business tax parameters: {&quot;CIT_rate&quot;: [{&quot;value&quot;: 0.25, &quot;year&quot;: 2021}]} | . After clicking run, three simulations will be created on Compute Studio and the app will update as soon as the simulations have finished: . . . Once they are done, the simulations are best viewed and interacted with on Compute Studio, but you can still inspect the JSON response from the Compute Studio API: . . I created this app to show that it’s possible to build apps on top of the Compute Studio API. I think PSL Stitch is a neat example of how to do this, but I am even more excited to see what others build next. . Also, this is an open source project and has lots of room for improvement. If you are interested in learning web technologies related to REST APIs and frontend development with JavaScript, then this project could be a good place to start! . Resources: . PSL Stitch | Source code | Compute Studio API Docs | .",
            "url": "https://blog.pslmodels.org/demo-day-9-cs-api-stitch",
            "relUrl": "/demo-day-9-cs-api-stitch",
            "date": " • Mar 8, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Demo Day: How to contribute to PSL projects",
            "content": ". In the most recent PSL Demo Day, I illustrate how to contribute to PSL projects. The open source nature of projects in the PSL catalog allows anyone to contribute. The modularity of the code, coupled with robust testing, means that one can bite off small pieces that help improve the models and remain confident those changes work as expected. . To begin the process of finding where to contribute to PSL projects, I advise looking through the PSL GitHub Organization to see what projects interest you. Once a project of interest is identified, looking over the open “Issues” can provide a sense of where model maintainers and users are looking for help (see especially the “Help Wanted” tags). It is also completely appropriate to create a new Issue to express interest in helping and ask for direction on where that might best be done given your experience and preferences. . When you are ready to begin to contribute to a project, you’ll want to fork and clone the GitHub repository to help you get the files on your local machine and ready for you to work with. Many PSL projects outline the detailed steps to get you up and running. For example, see the Tax-Calculator Contributor Guide, which outlines the step-by-step process for doing this and confirming that everything works as expected on your computer. . After you are set up and ready to begin modifying source code for the PSL project(s) you’re interested in contributing to, you can reference the PSL-incubating Git-Tutorial project that provides more details on the Git workflow followed by most PSL projects. . As you contribute, you may want to get more involved in the community. A couple ways to do this are to join any of the PSL community events, all of which are open to the public, and to post to the PSL Discourse Forums. These are great places to meet community members and ask questions about how and where to best contribute. . I hope this helps you get started as a PSL contributor – we look forward to getting you involved in making policy analysis better and more transparent! . Resources: . PSL Git-Tutorial | PSL community events | PSL Discourse Forums | Tax-Calculator Contributor Guide | PSL GitHub Organization | .",
            "url": "https://blog.pslmodels.org/demo-day-8-contributing-psl",
            "relUrl": "/demo-day-8-contributing-psl",
            "date": " • Mar 2, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Demo Day: Move Policy Reform Files from Tax-Brain to Tax-Cruncher",
            "content": "Check out the video: . . Show notes: . I demonstrate how to move a policy reform file from Tax-Brain to Tax-Cruncher using the Compute.Studio API. See the Demo C/S simulation linked below for text instructions that accompany the video. . Resources: . Demo C/S simulation with instructions | .",
            "url": "https://blog.pslmodels.org/demo-day-7-taxbrain-to-taxcruncher",
            "relUrl": "/demo-day-7-taxbrain-to-taxcruncher",
            "date": " • Mar 2, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Demo Day: Running the scf and microdf Python packages in Google Colab",
            "content": ". For Monday’s PSL Demo Day, I showed how to use the scf and microdf PSL Python packages from the Google Colab web-based Jupyter notebook interface. . The scf package extracts data from the Federal Reserve’s Survey of Consumer Finances, the canonical source of US wealth microdata. scf has a single function: load(years, columns), which then returns a pandas DataFrame with the specified column(s), each record’s survey weight, and the year (when multiple years are requested). . The microdf package analyzes survey microdata, such as that returned by the scf.load function. It offers a consistent paradigm for calculating statistics like means, medians, sums, and inequality statistics like the Gini index. Most functions are structured as follows: f(df, col, w, groupby) where df is a pandas DataFrame of survey microdata, col is a column(s) name to be summarized, w is the weight column, and groupby is the column(s) to group records in before summarizing. . Using Google Colab, I showed how to use these packages to quickly calculate mean, median, and total wealth from the SCF data, without having to install any software or leave the browser. I also demonstrated how to use the groupby argument of microdf functions to show how different measures of wealth inequality have changed over time. Finally, I previewed some of what’s to come with scf and microdf: imputations, extrapolations, inflation, visualization, and documentation, to name a few priorities. . Resources: . Slides | Demo notebook in Google Colab | Simulation from the demonstration | scf GitHub repo | microdf GitHub repo | microdf documentation | .",
            "url": "https://blog.pslmodels.org/demo-day-6-scf-microdf",
            "relUrl": "/demo-day-6-scf-microdf",
            "date": " • Jan 29, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Demo Day: How to Use OG-USA Macroeconomic Model of U.S. Fiscal Policy",
            "content": ". . . In this PSL Demo Day, I demonstrate how to use the open source OG-USA macroeconomic model of U.S. fiscal policy. Jason DeBacker and I (Richard Evans) have been the core maintainers of this project and repository for the last six years. This demo is organized into the following sections. The YouTube webinar linked above took place on January 11, 2021. . A brief note about value of the PSL community | Description of the OG-USA model | Using OG-USA on Compute Studio | . Brief note about the value of the PSL community . The Policy Simulation Library is a decentralized organization of open source policy models. The Policy Simulation Library GitHub organization houses many open source repositories, each of which represents a curated policy project by a diverse group of maintainers. The projects that have met the highest standards of best practices and documentation are designated as psl-cataloged, while newer projects that are in earlier stages are designated as psl-incubating. The philosophy and goal of the PSL environment is to make policy modeling open and transparent. It also allows more collaboration and cross-project contributions and interactions. . The Policy Simulation Library group has been holding these PSL Demo Day webinars since the end of 2020. The video of each webinar is available on the Policy Simulation Library YouTube channel. These videos are a great resource for learning the different models available in the PSL community, how the models interact, how to contribute to them, and what is on the horizon in their development. Also excellent in many of the PSL Demo Day webinars is a demonstration of how to use the models on the Compute Studio web application platform. . I have been a participant in and contributor to the PSL community since its inception. I love economic policy modeling. And I learned how sophisticated and complicated economic policy models can be. And any simulation can have hundreds of underlying assumptions, some of which may not be explicitly transparent. I think models that are used for public policy analysis have a philosophical imperative to be open source. This allows others to verify results and test sensitivity to assumptions. . Another strong benefit of open source modeling is that it is fundamentally apolitical. With proprietary closed-source policy models, an outside observer might criticize the results of the model based on the perceived political biases of the modeler or the sponsoring organization. With open-source models, a critic can be redirected to the underlying assumptions, structure, and content of the model. This is constructive criticism and debate that moves the science foreward. In the current polarized political environment in the U.S., open-source modeling can provide a constructive route for bipartisan cooperation and the democratization of computational modeling. Furthermore, open-source modeling and workflow encourages the widest forms of collaboration and contributions. . Description of OG-USA model . OG-USA is an open-source overlapping generations, dynamic general equilibrium, heterogeneous agent, macroeconomic model of U.S. fiscal policy. The GitHub repository for the OG-USA source code is github.com/PSLmodels/OG-USA. This repository contains all the source code and instructions for loading and running OG-USA and all of its dependencies on your local machine. We will probably do another PSL Demo Day on how to run OG-USA locally. This Demo Day webinar is about running OG-USA on the Compute Studio web application. See Section “Using OG-USA on Compute.Studio” below. . As a heterogeneous agent macroeconomic model, OG-USA allows for distributional analyses at the individual and firm level. That is, you can simulate the model and answer questions like, “How will an increase in the top three personal income tax rates affect people of different ages and income levels?” Microsimulation models can answer these types of distributional analysis questions as well. However, the difference between a macroeconomic model and a microsimulation model is that the macroeconomic models can simulate how each of those individuals and firms will respond to a policy change (e.g., lower labor supply or increased investment demand) and how those behavioral responses will add up and feed back into the macroeconomy (e.g., the effect on GDP, government revenue, government debt, interest rates, and wages). . OG-USA is a large-scale model and comprises tens of thousands of lines of code. The status of all of this code being publicly available on the internet with all collaboration and updates also public makes this an open source project. However, it is not enough to simply post one’s code. We have gone to great lengths to make in-line comments or “docstring” in the code to clarify the purpose of each function and line of code. For example, look in the OG-USA/ogusa/household.py module. The first function on line 18 is the marg_ut_cons() function. As is described in its docstring, its purpose is to “Compute the marginal utility of consumption.” . These in-code docstrings are not enough. We have also created textbook style OG-USA documentation at pslmodels.github.io/OG-USA/ using the Jupyter Book medium. This form of documentation has the advantage of being in book form and available online. It allows us to update the documentation in the open-source repository so changes and versions can be tracked. It describes the OG-USA API, OG-USA theory, and `OG-USA calibration. As with the model, this documentation is always a work in progress. But being open-source allows outside contributors to help with its updated and error checking. . One particular strength of the OG-USA model I want to highlight is its interaction with microsimulation models to incorporate information about tax incentives faced by the heterogeneous households in the model. We have interfaced OG-USA with microsimulation models in India and in the European Commission. OG-USA’s default for modeling the United States is to use the open-source Tax-Calculator microsimulation model, which was described by Anderson Frailey in the last Demo Day of 2020. However, DeBacker and I currently have a project in which we use OG-USA to simulate policies using the Tax Policy Center’s microsimulation model. The way OG-USA interfaces with microsimulation models to incorporate rich tax data is described in the documentation in the calibration chapter entitled, “Tax Functions”. . Using OG-USA on Compute Studio . In the demonstration, I focus on how to run experiments and simulations with OG-USA using the Compute Studio web application platform rather than installing running the model on your local machine. To use OG-USA on this web application, you will need a Compute Studio account. Once you have an account, you can start running any model available through the site. For some models, you will have to pay for the compute time, although the cost of running these models is very modest. However, all Compute Studio simulations of the OG-USA model are currently sponsored by the Open Source Economics Laboratory. This subsidy will probably run out in the next year. But we are always looking for funding for these models. . Once you are signed up and logged in to your Compute Studio account, you can go to the OG-USA model on Compute Studio at compute.studio/PSLmodels/OG-USA. The experiment that we simulated in the demonstration is available at compute.studio/PSLmodels/OG-USA/206. The description at the top of the simulation page describes the changes we made. You can look through the input page by clicking on the “Inputs” tab. We ran the model by clicking the green “Run” button at the lower left of the page. The model took about 5 hours to run, so I pre-computed the results that we discussed in the demo. The outputs of the experiment are available in the “Outputs” tab on the page. I also demonstrated how one can click the “Download Results” button at the bottom of the “Outputs” tab to download more results from the simulation. However, the full set of results is only available by installing and running the OG-USA model simulation on your local machine. . The benefits of the Compute Studio web application are that running the OG-USA model is much easier for the non-expert, and the multiple-hour computation time can be completed on a remote machine in the cloud. . Resources . PSL Demo Day YouTube webinar: “How to use OG-USA” | OG-USA on Compute Studio | Simulation from the demonstration | OG-USA GitHub repo | OG-USA documentation | Tax-Calculator GitHub repo | .",
            "url": "https://blog.pslmodels.org/demo-day-5-how-to-use-ogusa",
            "relUrl": "/demo-day-5-how-to-use-ogusa",
            "date": " • Jan 28, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Demo Day: Tax-Brain",
            "content": ". For this PSL demo-day I showed how to use the Tax-Brain web-application, hosted on Compute Studio, to analyze proposed individual income tax policies. Tax-Brain integrates the Tax-Calculator and Behavioral-Responses models to make running both static and dynamic analyses of the US federal income and payroll taxes simple. The web interface for the model makes it possible for anyone to run their own analyses without writing a single line of code. . We started the demo by simply walking through the interface and features of the web-app before creating our own sample reform to model. This reform, which to my knowledge does not reflect any proposals currently up for debate, included changes to the income and payroll tax rates, bringing back personal exemptions, modifying the standard deduction, and implementing a universal basic income. . While the model ran, I explained how Tax-Brain validated all of the user inputs, the data behind the model, and how the final tax liability projections are determined. We concluded by looking through the variety of tables and graphs Tax-Brain produces and how they can easily be shared with others. . Resources: . Simulation from the demonstration | Tax-Brain GitHub repo | Tax-Calculator documentation | Behavioral-Responses documentation | .",
            "url": "https://blog.pslmodels.org/demo-day-4-taxbrain",
            "relUrl": "/demo-day-4-taxbrain",
            "date": " • Dec 23, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "2020: A Year in Review",
            "content": ". . This year has been one to forget! But 2020 did have its bright spots, especially in the PSL community. This post reviews some of the highlights from the year. . The Library was able to welcome two new models to the catalog in 2020: microdf and OpenFisca-UK. microdf provides a number of useful tools for use with economic survey data. OpenFisca-UK builds off the OpenFisca platform, offering a microsimulation model for tax and benefit programs in the UK. . In addition, four new models were added to the Library as incubating projects. The ui-calculator model has received a lot of attention this year in the U.S., as it provides the capability to calculate unemployment insurance benefits across U.S. states, a major mode of delivering financial relief to individuals during the COVID crisis. PCI-Outbreak directly relates to the COVID crisis, using machine learning and natural language processing to estimate the true extent of the COVID pandemic in China. The model finds that actual COVID cases are significantly higher than what official statistics claim. The COVID-MCS model considers COVID case counts and test positivity rates to measure whether or not U.S. communities are meeting certain benchmarks in controlling the spread of the disease. On a lighter note, the Git-Tutorial project provides instruction and resources for learning to use Git and GitHub, with an emphasis on the workflow used by many projects in the PSL community. . The organization surrounding the Policy Simulation Library has been bolstered in two ways. First, we have formed a relationship with the Open Collective Foundation, who is now our fiscal host. This allows PSL to accept tax deductible contributions that will support the efforts of the community. Second, we’ve formed the PSL Foundation, with an initial board that includes Linda Gibbs, Glenn Hubbard, and Jason DeBacker. . Our outreach efforts have grown in 2020 to include the regular PSL Demo Day series and this PSL Blog. Community members have also presented work with PSL models at the PyData Global Conference, the Tax Economists Forum, AEI, the Coiled Podcast, and the Virtual Global Village Podcast. New users will also find a better experience learning how to use and contribute to PSL models as many PSL models have improved their documentation through the use of Jupyter Book (e.g., see the Tax-Calculator documentation). . We love seeing the community around open source policymaking expand and are proud that PSL models have been used for important policy analysis in 2020, including analyzing economic policy responses to the pandemic and the platforms of presidential candidates. We look forward to more progress in 2021 and welcome you to join the effort as a contributor, financially or as an open source developer. . Best wishes from PSL for a happy and healthy new year! . Resources: . PSL Twitter Feed | PSL YouTube | PSL on Open Collective | .",
            "url": "https://blog.pslmodels.org/2020-year-in-review",
            "relUrl": "/2020-year-in-review",
            "date": " • Dec 23, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Demo Day: Cost-of-Capital-Calculator Web Application",
            "content": ". In the PSL Demo Day video linked above, I demonstrate how to use the Cost-of-Capital-Calculator (CCC) web application on Compute-Studio. CCC computes various measures of the impact of the tax system on business investment. These include the Hall-Jorgenson cost of capital, marginal effective tax rates, and effective average tax rates (following the methodology of Devereux and Griffith (1999)). . I begin by illustrating the various parameters available for the user to manipulate. These include parameters of the business and individual income tax systems, as well as parameters representing economic assumptions (e.g., inflation rates and nominal interest rates) and parameters dictating financial and accounting policy (e.g., the fraction of financing using debt). Note that all default values for tax policy parameters represent the “baseline policy”, which is defined as the current law policy in the year being analyzed (which itself is a parameter the user can change). Other parameters are estimated using historical data following the methodology of CBO (2014). . Next, I change a few parameters and run the model. In this example, I move the corporate income tax rate up to 28% and lower bonus depreciation for assets with depreciable lives of 20 years or less to 50%. . Finally, I discuss how to interpret output. The web app returns a table and three figures summarizing marginal effective total tax rates on new investments. This selection of output helps give one a sense of the the overall changes, as well as effects across asset types, industries, and type of financing. For the full model output, one can click on “Download Results”. Doing so will download four CSV files contain several measures of the impact of the tax system on investment for very fine asset and industry categories. Users can take these files and create tables and visualizations relevant to their own use case. . Please take the model for a spin and simulate your own reform. If you have questions, comments, or suggestions, please let me know on the PSL Discourse (non-technical questions) or by opening an issue in the CCC GitHub repository (technical questions). . Resources: . Compute Studio simulation used in the demonstration | Cost-of-Capital-Calculator web app | Cost-of-Capital-Calculator documentation | Cost-of-Capital-Calculator GitHub repository | .",
            "url": "https://blog.pslmodels.org/demo-day-3-cost-of-capital-calculator",
            "relUrl": "/demo-day-3-cost-of-capital-calculator",
            "date": " • Dec 3, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Demo Day: Tax-Cruncher",
            "content": ". For the Demo Day on November 16, I showed how to calculate a taxpayer’s liabilities under current law and under a policy reform with Tax-Cruncher. The Tax-Cruncher web application takes two sets of inputs: a taxpayer’s demographic and financial information and the provisions of a tax reform. . For the first Demo Day example (3:50), we looked at how eliminating the state and local tax (SALT) deduction cap and applying payroll tax to earnings above $400,000 would affect a high earner. In particular, our hypothetical filer had $500,000 in wages, $100,000 in capital gains, and $100,000 in itemizable expenses. You can see the results at Compute Studio simulation #634. . For the second example (17:50), we looked at how expanding the Earned Income Tax Credit (EITC) and Child Tax Credit would impact a family with $45,000 in wages and two young children. You can see the results at Compute Studio simulation #636. . Resources: . Tax-Cruncher | Tax-Cruncher-Biden | .",
            "url": "https://blog.pslmodels.org/demo-day-2-tax-cruncher",
            "relUrl": "/demo-day-2-tax-cruncher",
            "date": " • Nov 23, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "Demo Day: Building Policy Reform Files",
            "content": "Check out the video: . We will host Demo Days every two weeks until the end of the year. You can see our schedule on our events page. . . Show notes: . I demonstrate how to build policy reform files using the Tax-Brain webapp on Compute Studio. (Useful links below.) This is an introductory lesson that ends with a cliffhanger. We don’t run the model. But we do generate an individual income and payroll tax reform file that is compatible with a range of policy simulation models and analytic tools, some designed for policy decision makers, others for taxpayers and benefits recipients interested in assessing their own circumstances. . Beyond individual and payroll tax analysis, the reform file can be used with models that assess pass-through and corporate taxation of businesses, as well as a variety of income benefit programs. A wide range of use cases will occupy future events. . Resources: . Demo C/S simulation | IRS Form 1040 | PSL Catalog | PSL Events | .",
            "url": "https://blog.pslmodels.org/demo-day-1-creating-reform-files",
            "relUrl": "/demo-day-1-creating-reform-files",
            "date": " • Nov 18, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "Introducing the PSL Blog",
            "content": "Our mission at the Policy Simulation Library is to improve public policy by opening up models and data preparation routines for policy analysis. To support and showcase our diverse community of users and developers, we engage across several mediums: a monthly newsletter, a Q&amp;A forum, (now-virtual) meetups, our Twitter feed, our YouTube channel, documentation for models in our catalog, and of course, issues and pull requests on GitHub. . Today, we’re adding a new medium: the PSL Blog. We’ll use this space to share major updates on our catalog, provide tutorials, and summarize events or papers that involve our models. . If you’d like to share your work on our blog, or to suggest content, drop me a line. To follow along, add the PSL blog’s RSS feed or subscribe to our newsletter. . Happy reading, . Max Ghenis . Editor, PSL Blog .",
            "url": "https://blog.pslmodels.org/introducing-psl-blog",
            "relUrl": "/introducing-psl-blog",
            "date": " • Nov 6, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "The Policy Simulation Library (PSL) is a collection of models and other software for public-policy decisionmaking. PSL is developed by independent projects that meet standards for transparency and accessibility. The PSL community encourages collaborative contribution and makes the tools it develops accessible to a diverse group of users.1 . This website is powered by fastpages, a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://blog.pslmodels.org/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://blog.pslmodels.org/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}
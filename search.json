[
  {
    "objectID": "posts/2021-03-02-demo-day-contributing-psl.html",
    "href": "posts/2021-03-02-demo-day-contributing-psl.html",
    "title": "Demo Day: Contributing to PSL projects",
    "section": "",
    "text": "In the most recent PSL Demo Day, I illustrate how to contribute to PSL projects. The open source nature of projects in the PSL catalog allows anyone to contribute. The modularity of the code, coupled with robust testing, means that one can bite off small pieces that help improve the models and remain confident those changes work as expected.\nTo begin the process of finding where to contribute to PSL projects, I advise looking through the PSL GitHub Organization to see what projects interest you. Once a project of interest is identified, looking over the open “Issues” can provide a sense of where model maintainers and users are looking for help (see especially the “Help Wanted” tags). It is also completely appropriate to create a new Issue to express interest in helping and ask for direction on where that might best be done given your experience and preferences.\nWhen you are ready to begin to contribute to a project, you’ll want to fork and clone the GitHub repository to help you get the files on your local machine and ready for you to work with. Many PSL projects outline the detailed steps to get you up and running. For example, see the Tax-Calculator Contributor Guide, which outlines the step-by-step process for doing this and confirming that everything works as expected on your computer.\nAfter you are set up and ready to begin modifying source code for the PSL project(s) you’re interested in contributing to, you can reference the PSL-incubating Git-Tutorial project that provides more details on the Git workflow followed by most PSL projects.\nAs you contribute, you may want to get more involved in the community. A couple ways to do this are to join any of the PSL community events, all of which are open to the public, and to post to the PSL Discourse Forums. These are great places to meet community members and ask questions about how and where to best contribute.\nI hope this helps you get started as a PSL contributor – we look forward to getting you involved in making policy analysis better and more transparent!\nResources:\n\nPSL Git-Tutorial\nPSL community events\nPSL Discourse Forums\nTax-Calculator Contributor Guide\nPSL GitHub Organization"
  },
  {
    "objectID": "posts/2021-09-20-demo-day-cs-auto-deploy.html",
    "href": "posts/2021-09-20-demo-day-cs-auto-deploy.html",
    "title": "Demo Day: Deploying apps on Compute Studio",
    "section": "",
    "text": "Compute Studio (C/S) is a platform for publishing and sharing computational models and data visualizations. In this demo day, I show how to publish your own project on C/S using the new automated deployments feature. You can find an in depth guide to publishing on C/S in the developer docs.\nC/S supports two types of projects: models and data visualizations. Models are fed some inputs and return a result. Data visualizations are web servers backed by popular open-source libraries like Bokeh, Dash, or Streamlit. Models are good for long-running processes and producing archivable results that can be shared and returned to easily. Data visualizations are good for highly interactive and custom user experiences.\nNow that you’ve checked out the developer docs and set up your model or data-viz, you can head over to the C/S publishing page https://compute.studio/new/ to publish your project. Note that this page is still very much under construction and may look different in a few weeks.\n\n\n\nPublish page\n\n\nNext, you will be sent to the second stage in the publish flow where you will provide more details on how to connect your project on C/S:\n\n\n\nConnect Project page\n\n\nClicking “Connect App” will take you to the project home page:\n\n\n\nProject home page\n\n\nGo to the “Settings” button in the top-right corner and this will take you to the project dashboard where you can modify everything from the social preview of your project to the amount of compute resources it needs:\n\n\n\nProject dashboard\n\n\nThe “Builds” link in the sidebar will take you to the builds dashboard where you can create your first build:\n\n\n\nBuild history dashboard\n\n\nIt’s time to create the first build. You can do so by clicking “New Build”. This will take you to the build status page. While the build is being scheduled, the page will look like this:\n\n\n\nBuild scheduled page\n\n\nYou can click the “Build History” link and it will show that the build has been started:\n\n\n\nBuild history dashboard\n\n\nThe build status page should be updated at this point and will look something like this:\n\n\n\nBuild status page\n\n\nC/S automated deployments are built on top of Github Actions. Unfortunately, the logs in Github Actions are not available through the Github API until after the workflow is completely finished. The build status dashboard will update as the build progresses and once it’s done, you will have full access to the logs from the build. These will contain outputs from installing your project and the outputs from your project’s tests.\nIn this case, the build failed. We can inspect the logs to see that an import error caused the failure:\n\n\n\nBuild failed status page\n\n\n\n\n\nBuild failed status page with logs\n\n\nI pushed an update to my fork of Tax-Cruncher on Github and restarted the build by clicking “Failure. Start new Build”. The next build succeeded and we can click “Release” to publish the project:\n\n\n\nBuild status page success\n\n\nThe builds dashboard now shows the two builds:\n\n\n\nUpdated build history page\n\n\nFinally, let’s go run our new model:\n\n\n\nRun project page\n\n\nIt may take a few seconds for the page to load. This is because the model code and all of its dependencies are being loaded onto the C/S servers for the first time:\n\n\n\nRun project page with inputs form\n\n\nThe steps for publishing a data visualization are very similar. The main idea is that you tell C/S what Python file your app lives in and C/S will know how to run it given your data visualization technology choice."
  },
  {
    "objectID": "posts/2020-11-06-introducing-psl-blog.html",
    "href": "posts/2020-11-06-introducing-psl-blog.html",
    "title": "Introducing the PSL Blog",
    "section": "",
    "text": "Our mission at the Policy Simulation Library is to improve public policy by opening up models and data preparation routines for policy analysis. To support and showcase our diverse community of users and developers, we engage across several mediums: a monthly newsletter, a Q&A forum, (now-virtual) meetups, our Twitter feed, our YouTube channel, documentation for models in our catalog, and of course, issues and pull requests on GitHub.\nToday, we’re adding a new medium: the PSL Blog. We’ll use this space to share major updates on our catalog, provide tutorials, and summarize events or papers that involve our models.\nIf you’d like to share your work on our blog, or to suggest content, drop me a line. To follow along, add the PSL blog’s RSS feed or subscribe to our newsletter.\nHappy reading,\nMax Ghenis\nEditor, PSL Blog"
  },
  {
    "objectID": "posts/2020-11-18-demo-day-creating-reform-files.html",
    "href": "posts/2020-11-18-demo-day-creating-reform-files.html",
    "title": "Demo Day: Building policy reform files",
    "section": "",
    "text": "Check out the video:\n\nWe will host Demo Days every two weeks until the end of the year. You can see our schedule on our events page.\n\nShow notes:\nI demonstrate how to build policy reform files using the Tax-Brain webapp on Compute Studio. (Useful links below.) This is an introductory lesson that ends with a cliffhanger. We don’t run the model. But we do generate an individual income and payroll tax reform file that is compatible with a range of policy simulation models and analytic tools, some designed for policy decision makers, others for taxpayers and benefits recipients interested in assessing their own circumstances.\nBeyond individual and payroll tax analysis, the reform file can be used with models that assess pass-through and corporate taxation of businesses, as well as a variety of income benefit programs. A wide range of use cases will occupy future events.\nResources:\n\nDemo C/S simulation\nIRS Form 1040\nPSL Catalog\nPSL Events"
  },
  {
    "objectID": "posts/2021-03-08-demo-day-cs-api-stitch.html",
    "href": "posts/2021-03-08-demo-day-cs-api-stitch.html",
    "title": "Demo Day: Stitching together apps on Compute Studio",
    "section": "",
    "text": "In Demo Day 8, I talked about connecting multiple apps on Compute Studio with PSL Stitch. The source code for PSL stitch can be found in this repository.\nStitch is composed of three components:\n\nA python package that can be run like a normal Python package.\nA RESTful API built with FastAPI that is called remotely to create simulations on Compute Studio.\nA GUI built with ReactJS that makes calls to the REST API to create and monitor simulations.\n\nOne of the cool things about this app is that it uses ParamTools to read the JSON files under the hood. This means that it can read links to data in other Compute Studio runs, files on GitHub, or just plain JSON. Here are some example parameters:\n\npolicy parameters: cs://PSLmodels:Tax-Brain@49779/inputs/adjustment/policy\ntax-cruncher parameters: {\"sage\": [{\"value\": 25}]}\nbusiness tax parameters: {\"CIT_rate\": [{\"value\": 0.25, \"year\": 2021}]}\n\nAfter clicking run, three simulations will be created on Compute Studio and the app will update as soon as the simulations have finished:\n\n\n\nGetting started\n\n\n\n\n\nCS Simulations\n\n\nOnce they are done, the simulations are best viewed and interacted with on Compute Studio, but you can still inspect the JSON response from the Compute Studio API:\n\n\n\nSimulation Complete\n\n\nI created this app to show that it’s possible to build apps on top of the Compute Studio API. I think PSL Stitch is a neat example of how to do this, but I am even more excited to see what others build next.\nAlso, this is an open source project and has lots of room for improvement. If you are interested in learning web technologies related to REST APIs and frontend development with JavaScript, then this project could be a good place to start!\nResources:\n\nPSL Stitch\nSource code\nCompute Studio API Docs"
  },
  {
    "objectID": "posts/2023-12-28-2023-year-in-review.html",
    "href": "posts/2023-12-28-2023-year-in-review.html",
    "title": "2023: A year in review",
    "section": "",
    "text": "While there haven’t been any blog posts in 2023 :wink:, it has been a productive year for the Policy Simulation Library (PSL) community and PSL Foundation!\nWe’ve continued to serve our mission through education and outreach efforts. We hosted 13 Demo Days in 2023, including presentations from individuals at the Congressional Budget Office, Allegheny County, NOAA, Johns Hopkins, QuantEcon, the City of New York, and other institutions. Archived videos of the Demo Days are available on our YouTube Channel.\nIn addition, we hosted an in person workshop at the National Tax Association’s annual conference in November. This event featured the PolicyEngine-US project and was lead by Max Ghenis and Nikhil Woodruff, co-founders of PolicyEngine. Attendees included individuals from the local area (Denver) and conference attendees, who represented academia, government, and think tanks. Max and Nikhil provided an overview of PolicyEngine and then walked attendees through a hands-on exercise using the PolicyEngine US tool, having them write code to generate custom plots in a Google Colab notebook. It was a lot of fun – and the pizza was decent too!\nSpeaking of PolicyEngine, this fiscally-sponsored project of PSL Foundation had a banner year in terms of fundraising and development. The group received several grants in 2023 and closed out the year with a large grant from Arnold Ventures. They also wrote an NSF grant proposal which they are waiting to hear back from. The group added an experienced nonprofit executive, Leigh Gibson, to their team. Leigh provides support with fundraising and operations, and she’s been instrumental in these efforts. In terms of software development, the PolicyEngine team has been able to greatly leverage volunteers (more than 60!) with Pavel Makarchuk coming on as Policy Modeling Manager to help coordinate these efforts. With their community, PolicyEngine has codified numerous US state tax and benefit policies and has developed a robust method to create synthetic data for use in policy analysis. Be on the lookout for a lot more from them in 2024.\nQuantEcon, another fiscally sponsored project, has also made tremendous contributions to open source economics in 2023. Most importantly, they ran a very successful summer school in West Africa. In addition, they have continued make key contributions to software tools useful for teaching and training economics tools. These include Jupyteach, which Spencer Lyon shared in our Demo Day series. With their online materials, textbooks, and workshops around the world, QuantEcon is shaping how researchers and policy analysts employ economic tools to solve real-world problems.\nPSL Foundation added a third fiscally sponsored project, Policy Change Index (PCI) in 2023. PCI was founded by Weifeng Zhong, a Senior Research Fellow at the Mercatus Center at George Mason University, and uses natural language processing and machine learning to predict changes in policy among autocratic regimes. PCI has had a very successful start with PCI-China, predicting policy changes in China, and PCI-Outbreak, predicting the extent of true COVID-19 case counts in China during the pandemic. Currently, they are extending their work to include predictive indices for Russia, North Korea, and Iran. PSL-F is excited for the opportunity to help support this important work.\nOther cataloged projects have continued to be widely used in 2023. To note a few of these use cases, the United Nations has partnered with Richard Evans and Jason DeBacker, maintainers of OG-Core, to help bring the modeling platform to developing countries they are assisting. Tax Foundation’s Capital Cost Recovery model has been updated to 2023 and used in their widely cited 2023 Tax Competitiveness Index. And the Tax-Calculator and TaxData projects both continue to used by think tanks and researchers.\nAs 2023 comes to a close, we look forward to 2024. We’ll be launching a new PSLmodels.org website soon. And there’ll be many more events – we hope you join in.\nFrom all of us at the PSL, best wishes for a happy and healthy New Year!\nResources:\n\nPSL Models\nPSL Foundation\nPSL Twitter Feed\nPSL YouTube channel\nPSL on Open Collective\nPSL Shop for PSL branded merchandise"
  },
  {
    "objectID": "posts/2022-04-12-demo-day-policyengine-us.html",
    "href": "posts/2022-04-12-demo-day-policyengine-us.html",
    "title": "Demo Day: Modeling taxes and benefits with the PolicyEngine US web app",
    "section": "",
    "text": "PolicyEngine is a nonprofit that builds free, open-source software to compute the impact of public policy. After launching our UK app in October 2021, we’ve just launched our US app, which calculates households’ federal taxes and several benefit programs, both under current law and under customizable policy reforms.\nIn this Demo Day, I provide background on PolicyEngine and demonstrate how to use PolicyEngine US (a Policy Simulation Library cataloged model) to answer a novel policy question:\n\nHow would doubling both (a) the Child Tax Credit and (b) the Supplemental Nutrition Assistance Program (SNAP) net income limit affect a single parent in California with $1,000 monthly rent and $50 monthly broadband costs?\n\nBy bringing together tax and benefit models into a web interface, we can answer this question quickly without programming experience, as well as an unlimited array of questions like it. The result is a table breaking down the household’s net income by program, as well as graphs of net income and marginal tax rates as the household’s earnings vary.\nI close with a quick demo of PolicyEngine UK, which adds society-wide results like the impact of reforms on the budget, poverty, and inequality, as well as contributed policy parameters. We’re planning to bring those features to PolicyEngine US, along with state tax and benefit programs in all 50 states, over the next two years (if not sooner).\nFeel free to explore the app and reach out with any questions at max@policyengine.org.\nResources:\n\nPolicyEngine US\nPresentation slides\nPolicyEngine blog post on launching PolicyEngine US"
  },
  {
    "objectID": "posts/2022-04-18-demo-day-ccc-international.html",
    "href": "posts/2022-04-18-demo-day-ccc-international.html",
    "title": "Demo Day: Analyzing tax competitiveness with Cost-of-Capital-Calculator",
    "section": "",
    "text": "In the Demo Day video shared here, I show how to use open source tools to analyze international corporate tax competitiveness. The two main tools illustrated are the Cost-of-Capital-Calculator (CCC), a model to compute measures of the tax burden on new investments, and Tax Foundation’s International Tax Competitiveness Index (ITCI).\nTax Foundation has made many helpful resources available online. Their measures of international business tax policy are a great example of this. The ICTI outputs and inputs are all well documented, with source code to reproduce results available on GitHub.\nI plug Tax Foundation’s country-by-country data into CCC functions using it’s Python API. Because CCC is designed to flexibly take array or scalar data, operating on rows of tabular data, such as that in the ITCI, is relatively straight-forward. The Google Colab notebook I walk through in this Demo Day, can be a helpful example to follow if you’d like to do something similar to this with the Tax Foundation data - or your own data source. From the basic building blocks there (reading in data, calling CCC functions), you can extend the analysis in a number of ways. For example adding additional years of data (Tax Foundation posts their data back to 2014), modifying economic assumptions, or creating counter-factual policy experiments across sets of countries.\nIf you find this example useful, or have questions or suggestions about this type of analysis, please feel free to reach out to me.\nResources:\n\nColab Notebook\nTax Foundation International Tax Competitiveness Index 2021\nGitHub repo for Tax Foundation ITCI data\nCost-of-Capital-Calculator documentation"
  },
  {
    "objectID": "posts/2021-06-14-demo-day-tax-brain-python-api.html",
    "href": "posts/2021-06-14-demo-day-tax-brain-python-api.html",
    "title": "Demo Day: Using the TaxBrain Python API",
    "section": "",
    "text": "The TaxBrain project was primarily created to serve as the backend of the Tax-Brain web-application. But at its core, TaxBrain is a Python package that greatly simplifies tax policy analysis. For this PSL Demo-Day, I demonstrated TaxBrain’s capabilities as a standalone package, and how to use it to produce high-level summaries of the revenue impacts of proposed tax policies. The Jupyter Notebook from the presentation can be found here.\nTaxBrain’s Python API allows you to run a full analysis of income tax policies in just three lines of code:\nfrom taxbrain import TaxBrain\n\ntb = TaxBrain(START_YEAR, END_YEAR, use_cps=True, reform=REFORM_POLICY)\ntb.run()\nWhere START_YEAR and END_YEAR are the first and last years, respectively, of the analysis; use_cps is a boolean indicator that you want to use the CPS-based microdata file prepared for use with Tax-Calculator; and REFORM_POLICY is either a JSON file or Python dictionary that specifies a reform suitable for Tax-Calculator. The forthcoming release of TaxBrain will also include a feature that allows you to perform a stacked revenue analysis as well. The inspiration for this feature was presented by Jason DeBacker in a previous demo-day.\nOnce TaxBrain has been run, there are a number of methods and functions included in the package to create tables and plots to summarize the results. I used the Biden 2020 campaign proposal in the demo and the resulting figures are below. The first is a “volcano plot” that makes it easy to see the magnitude of the change in tax liability individuals across the income distribution face. Each dot represents a tax unit, and the x and y variables can be customized based on the user’s needs.\n\nThe second gives a higher-level look at how taxes change in each income bin. It breaks down what percentage of each income bin faces a tax increase or decrease, and the size of that change.\n\nThe final plot shown in the demo simply shows tax liabilities by year over the budget window.\n\nThe last feature I showed was TaxBrain’s automated reports. TaxBrain uses saved results and an included report template to write a report summarizing the findings of your simulation. The reports include tables and figures similar to what you may find in similar write ups by the Joint Committee on Taxation or Tax Policy Center including a summary of significant changes caused by the reform, and all you need is one line of code:\nreport(tb, name='Biden Proposal', outdir='biden', author='Anderson Frailey')\nThe above code will save a PDF copy of the report in a directory called biden along with PNG files for each of the graphs created and the raw Markdown text used for the report, which you can then edit as needed if you would like to add content to the report that is not already included. Screenshots of the default report are included below.\n    \nThere are of course downsides to using TaxBrain instead of Tax-Calculator directly. Specifically, it’s more difficult, and sometimes impossible, to perform custom tasks like modeling a feature of the tax code that hasn’t been added to Tax-Calculator yet or advanced work with marginal tax rates. But for day-to-day tax modeling, the TaxBrain Python package can significantly simply any workflow.\nResources:\n\nTax-Brain GitHub repo\nTax-Brain Documentation"
  },
  {
    "objectID": "posts/2021-03-02-demo-day-taxbrain-to-taxcruncher.html",
    "href": "posts/2021-03-02-demo-day-taxbrain-to-taxcruncher.html",
    "title": "Demo Day: Moving policy reform files from Tax-Brain to Tax-Cruncher",
    "section": "",
    "text": "Check out the video:\n\n\nShow notes:\nI demonstrate how to move a policy reform file from Tax-Brain to Tax-Cruncher using the Compute.Studio API. See the Demo C/S simulation linked below for text instructions that accompany the video.\nResources:\n\nDemo C/S simulation with instructions"
  },
  {
    "objectID": "posts/2021-12-28-2021-year-in-review.html",
    "href": "posts/2021-12-28-2021-year-in-review.html",
    "title": "2021: A year in review",
    "section": "",
    "text": "As 2021 winds down, I wanted to take a few minutes to reflect on the Policy Simulation Library’s efforts over the past year. With an amazing community of contributors, supporters, and users, PSL has been able to make a real impact in 2021.\nThe library saw two new projects achieve “cataloged” status: Tax Foundation’s Capital Cost Recovery model and the Federal Reserve Bank of New York’s DSGE.jl model. Both models satisfy all the the PSL criteria for transparency and reproducibility. Both are also written entirely in open source software: the Capital Cost Recovery model is in R and the DSGE model in Julia.\nAn exciting new project to join the Library this year is PolicyEngine. PolicyEngine is building open source tax and benefit mircosimulation models and very user-friendly interfaces to these models. The goal of this project is to take policy analysis to the masses through intuitive web and mobile interfaces for policy models. The UK version of the PolicyEngine app has already seen use from politicians interested in reforming the tax and benefit system in the UK.\nAnother excellent new addition to the library is the Federal-State Tax Project. This project provides data imputation tools to allow for state tax data that are representative of each state as well as federal totals. These datasets can then be used in microsimulation models, such as Tax-Calculator to study the impact of federal tax laws across the states. Matt Jensen and Don Boyd have published several pieces with these tools, including in State Tax Notes\nPSL Foundation became an official business entity in 2021. While still awaiting a letter of determination for 501(c)(3) status from the IRS, PSL Foundation was able to raise more than $25,000 in the last few months of 2021 to support open source policy analysis!\nPSL community members continued to interact several times each week in our public calls. The PSL Shop was launched in 2021 so that anyone can get themselves some PSL swag (with some of each purchase going back to the PSL Foundation to support the Library). In addition, PSL hosted 20 Demo Day presentations from 11 different presenters! These short talks covered everything from new projects to interesting applications of some of the first projects to join the Library, as well as general open source tools.\nAs in past years, PSL cataloged and incubating models were found to be of great use in current policy debates. Whether it was the ARPA, Biden administration proposals to expand the CTC, or California’s Basic Income Bill, the accessibility and ability to reproduce results from these open source projects has made them a boon to policy analysts.\nWe are looking forward to a great 2022! We expect the Library to continue to grow, foresee many interesting and helpful Demo Days, and are planning a DC PSL Workshop for March 2022. We hope to see you around these or other events!\nBest wishes from PSL for a happy and healthy New Year!\nResources:\n\nPSL Foundation\nPSL Twitter Feed\nPSL YouTube channel\nPSL on Open Collective"
  },
  {
    "objectID": "posts/2022-06-28-demo-day-github.html",
    "href": "posts/2022-06-28-demo-day-github.html",
    "title": "Demo Day: Getting Started with GitHub",
    "section": "",
    "text": "Git and GitHub often present themselves as barriers to entry to would-be contributors to PSL projects, even for those who are otherwise experienced with policy modeling. But these tools are critical to collaboration on open source projects. In the Demo Day video linked above, I cover some of the basics to get set up and begin contributing to an open source project.\nThere are four steps I outline:\n\nCreate a “fork” of the repository you are interested in. A fork is a copy of the source code that resides on GitHub (i.e., in the cloud). A fork gives you control over a copy of the source code. You will be able to merge in changes to the code on this fork, even if you don’t have permissions to do so with the original repository.\n“Clone” the fork. Cloning will download a copy of the source code from your fork onto your local machine. But cloning is more than just downloading the source code. It will include the version history of the code and automatically create a link between the local files and the remote files on your fork.\nConfigure your local files to talk to both your fork (which has a default name of origin) and the original repository you forked from (which typically has the default name of upstream). Do this by using your command prompt or terminal to navigate to the directory you just cloned. From there, run:\n\ngit remote add upstream URL_to_original_repo.git\nAnd check that this worked by giving the command:\ngit remote -v\nIf things worked, you should see URLs to your fork and the upstream repository with “(fetch)” and “(push)” by them More info on this is in the Git docs.\n\nNow that you have copies of the source code on your fork and on your local machine, you are ready to begin contributing. As you make changes to the source code, you’ll want to work on development branches. Branches are copies of the code. Ideally, you keep your “main” (or “master”) branch clean (i.e., your best version of the code) and develop the code on branches. When you’ve completed the development work (e.g., adding a new feature) you will them merge this into the “main” branch.\n\nI hope this helps you get started contributing to open source projects. Git and GitHub are valuable tools and there is lots more to learn, but these basics will get you going. For more information, see the links below. If you want to get started working with a project in the Library, feel free to reach out to me through the relevant repo (@jdebacker on GitHub) or drop into a PSL Community Call (dates on the PSL Calendar).\nResources:\n\nPSL Git Tutorial\nGit Basics"
  },
  {
    "objectID": "posts/2022-07-14-demo-day-cambridge-cash-assistance.html",
    "href": "posts/2022-07-14-demo-day-cambridge-cash-assistance.html",
    "title": "Demo Day: How does targeted cash assistance affect incentives to work?",
    "section": "",
    "text": "In this week’s Demo Day, I shared my paper published at the Center for Growth and Opportunity in June. “How does targeted cash assistance affect incentives to work?” analyzed a program Mayor Sumbul Siddiqui proposed in Cambridge, Massachusetts to provide $500 per month for 18 months to all families with dependents and income below 200% of the poverty line.\nTargeted programs like these are common in guaranteed income pilots, and in some enacted policies, and I find that it would cost-effectively reduce poverty: if expanded to Massachusetts, it would cost $1.2 billion per year and cut child poverty 42%.\nHowever, that targeting comes at a cost. Using the OpenFisca US microsimulation model (supported by the Center for Growth and Opportunity and cataloged by the Policy Simulation Library), I find that the program would deepen an existing welfare cliff at 200% of the poverty line. For example, a family of four would lose over $19,000 total—$9,000 from the cash assistance and $10,000 from other benefits—once they earn a dollar above 200% of the poverty line (about $55,000). To recover those lost benefits, they would have to earn an additional $26,000, a range I call the “earnings dead zone”.\nMy presentation reviews these trends in both slides and the PolicyEngine US app for computing the impacts of tax and benefit policy. For example, I show how repealing the SNAP emergency allotment would smooth out welfare cliffs, while reducing resources available to low-income families, and how a universal child allowance avoids work disincentives while less cost-effectively reducing poverty.\nPolicymakers face trade-offs between equity and efficiency, and typically labor supply responses consider marginal tax rates. With their infinite marginal tax rates, welfare cliffs are a less explored area, even though they surface in several parts of the tax and benefit system. This paper makes a start, but more research is yet to be done."
  },
  {
    "objectID": "posts/2022-12-31-2022-year-in-review.html",
    "href": "posts/2022-12-31-2022-year-in-review.html",
    "title": "2022: A year in review",
    "section": "",
    "text": "This has been another successful year for the Policy Simulation Library, whose great community of contributors continue to make innovative advances in open source policy analysis, and for the PSL Foundation, which supports the Library and its community. We are so thankful for all those who have made financial or technical contributions to the PSL this year! In this blog post, I want to take this time at the end of the year to reflect on a few of the highlights from 2022.\nPolicyEngine, a PSL Foundation fiscally-sponsored project, launched PolicyEngine US in April and has since seen many use cases of the model (check out the PolicyEngine year-in-review here). PolicyEngine had begun by leveraging the OpenFisca platform, but has transitioned to their own-maintained PolicyEngine Core. PolicyEngine Core and their related projects (such as PolicyEngine US and PolicyEngine UK) already meet all the criteria set forth by the Policy Simulation Library. Keep an eye out for lots more excellent tax and benefit policy analysis tools from PolicyEngine in 2023 and beyond!\nPSL Foundation has partnered with QuantEcon, acting as a fiscal sponsor for their projects that provide training and training materials for economic modeling and econometrics using open source tools. QuantEcon ran a massive open online class in India that had more than 1000 registrants in summer of 2022. They also ran an online course for over 100 students from universities in Africa in 2022. Further, with the funding received through their partnership with PSL Foundation, QuantEcon will continue these efforts in 2023 with a planned, in-person course in India.\nPSL hosted its first in-person workshop in March. The workshop focused on open source tools for tax policy analysis including Tax-Calculator, Cost-of-Capital-Calculator, OG-USA, and PolicyEngine US. The PSL event was, appropriately enough, hosted at the MLK Memorial Library in DC. We filled the space with 30 attendees from think tanks, consultancies, and government agencies. The workshop was a great success and we look forward to hosting more in-person workshops in the future.\nPSL’s bi-weekly Demo Day series continued throughout 2022, with 13 Demo Days this year. In these, we saw a wide array of presenters from institutions such as the Federal Reserve Bank of Atlanta, PolicyEngine, Tax Foundation, National Center for Children in Poverty, IZA Institute of Labor Economics, Channels, the University of South Carolina, the Center for Growth and Opportunity, and the American Enterprise Institute. You can go back and rewatch any of these presentations on YouTube.\nIt’s been a fantastic year and we expect even more from the community and PSL Foundation in 2023. PSL community members continue to interact several times each week on our public calls. Check out the events page and join us in the New Year!\nFrom all of us at the PSL, best wishes for a happy and healthy New Year!\nResources:\n\nPSL Foundation\nPSL Twitter Feed\nPSL YouTube channel\nPSL on Open Collective\nPSL Shop for PSL branded merchandise"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "PSL Blog",
    "section": "",
    "text": "Updates from the Policy Simulation Library."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Policy Simulation Library Blog",
    "section": "",
    "text": "2023: A year in review\n\n\n\n\n\n\npsl\n\n\npsl-foundation\n\n\n\nHighlights from the Policy Simulation Library in 2023.\n\n\n\n\n\nDec 28, 2023\n\n\nJason DeBacker\n\n\n\n\n\n\n\n\n\n\n\n\n2022: A year in review\n\n\n\n\n\n\npsl\n\n\npsl-foundation\n\n\n\nHighlights from the Policy Simulation Library in 2022.\n\n\n\n\n\nDec 31, 2022\n\n\nJason DeBacker\n\n\n\n\n\n\n\n\n\n\n\n\nDemo Day: How does targeted cash assistance affect incentives to work?\n\n\n\n\n\n\ndemo-days\n\n\nbenefits\n\n\nus\n\n\n\nHow a proposed program in Cambridge, Massachusetts would affect poverty and incentives.\n\n\n\n\n\nJul 14, 2022\n\n\nMax Ghenis\n\n\n\n\n\n\n\n\n\n\n\n\nDemo Day: Getting Started with GitHub\n\n\n\n\n\n\ndemo-days\n\n\ngithub\n\n\ngit\n\n\nworkflow\n\n\ngetting-started\n\n\n\nThe basics of forking and cloning repositories and working on branches.\n\n\n\n\n\nJun 28, 2022\n\n\nJason DeBacker\n\n\n\n\n\n\n\n\n\n\n\n\nDemo Day: Analyzing tax competitiveness with Cost-of-Capital-Calculator\n\n\n\n\n\n\ndemo-days\n\n\ncost-of-capital-calculator\n\n\nbusiness-taxation\n\n\ncorporate-income-tax\n\n\ntaxes\n\n\n\nUsing Cost-of-Capital-Calculator with data on international business tax policies.\n\n\n\n\n\nApr 18, 2022\n\n\nJason DeBacker\n\n\n\n\n\n\n\n\n\n\n\n\nDemo Day: Modeling taxes and benefits with the PolicyEngine US web app\n\n\n\n\n\n\ndemo-days\n\n\napps\n\n\ntaxes\n\n\nbenefits\n\n\nus\n\n\n\nPolicyEngine US is a new web app for computing the impact of US tax and benefit policy.\n\n\n\n\n\nApr 12, 2022\n\n\nMax Ghenis\n\n\n\n\n\n\n\n\n\n\n\n\nPolicy Simulation Library DC Workshop: Open source tools for analyzing tax policy\n\n\n\n\n\n\nPSL\n\n\nPSL-Foundation\n\n\nWorkshop\n\n\n\nWashington, DC open-source modeling workshop, March 25, 2022, 8:30am-1:00pm, Martin Luther King, Jr. Memorial Library.\n\n\n\n\n\nMar 3, 2022\n\n\nJason DeBacker\n\n\n\n\n\n\n\n\n\n\n\n\n2021: A year in review\n\n\n\n\n\n\npsl\n\n\npsl-foundation\n\n\n\nHighlights from the Policy Simulation Library in 2021.\n\n\n\n\n\nDec 28, 2021\n\n\nJason DeBacker\n\n\n\n\n\n\n\n\n\n\n\n\nDemo Day: Using synthimpute for data fusion\n\n\n\n\n\n\ndemo-days\n\n\npython\n\n\ndata-fusion\n\n\nsynthimpute\n\n\n\nThe synthimpute Python package fuses and synthesizes economic datasets with machine learning.\n\n\n\n\n\nDec 8, 2021\n\n\nMax Ghenis\n\n\n\n\n\n\n\n\n\n\n\n\nDemo Day: The OG-Core platform\n\n\n\n\n\n\ndemo-days\n\n\npython\n\n\nmacroeconomics\n\n\noverlapping-generations\n\n\n\nA Python platform for building country-specific overlapping generations general equilibrium models.\n\n\n\n\n\nNov 1, 2021\n\n\nJason DeBacker\n\n\n\n\n\n\n\n\n\n\n\n\nDemo Day: Deploying apps on Compute Studio\n\n\n\n\n\n\ndemo-days\n\n\npolicy-simulation-library\n\n\ncompute-studio\n\n\n\nHow to deploy apps on Compute Studio using the new automated deployments feature.\n\n\n\n\n\nSep 20, 2021\n\n\nHank Doupe\n\n\n\n\n\n\n\n\n\n\n\n\nDemo Day: Unit testing for open source projects\n\n\n\n\n\n\ndemo-days\n\n\nR\n\n\nPython\n\n\nunit-testing\n\n\n\nHow to ensure that individual functions do what you expect.\n\n\n\n\n\nAug 9, 2021\n\n\nJason DeBacker\n\n\n\n\n\n\n\n\n\n\n\n\nDemo Day: Constructing tax data for the 50 states\n\n\n\n\n\n\ndemo-days\n\n\ntax\n\n\ndata\n\n\nus\n\n\n\nA new dataset to facilitate state-level analysis of federal tax reforms.\n\n\n\n\n\nJul 16, 2021\n\n\nDon Boyd\n\n\n\n\n\n\n\n\n\n\n\n\nDemo Day: Using the TaxBrain Python API\n\n\n\n\n\n\ndemo-days\n\n\nindividual-income-tax\n\n\ntax-brain\n\n\ntax-calculator\n\n\n\nA programmatic interface to compute the impact of tax reform.\n\n\n\n\n\nJun 14, 2021\n\n\nAnderson Frailey\n\n\n\n\n\n\n\n\n\n\n\n\nDemo Day: Updating Jupyter Book documentation with GitHub Actions\n\n\n\n\n\n\ndemo-days\n\n\njupyter-book\n\n\nGH-actions\n\n\ndocumentation\n\n\n\nHow to keep interactive programmatic notebook-based documentation up-to-date in your pull request workflow.\n\n\n\n\n\nMay 17, 2021\n\n\nJason DeBacker\n\n\n\n\n\n\n\n\n\n\n\n\nDemo Day: Producing stacked revenue estimates with the Tax-Calculator Python API\n\n\n\n\n\n\ndemo-days\n\n\nindividual-income-tax\n\n\ntax-brain\n\n\ntax-calculator\n\n\n\nHow to evaluate the cumulative effects of a multi-part tax reform.\n\n\n\n\n\nApr 5, 2021\n\n\nJason DeBacker\n\n\n\n\n\n\n\n\n\n\n\n\nDemo Day: Stitching together apps on Compute Studio\n\n\n\n\n\n\ndemo-days\n\n\nPSL\n\n\n\nCreating an app with the Compute Studio API.\n\n\n\n\n\nMar 8, 2021\n\n\nHank Doupe\n\n\n\n\n\n\n\n\n\n\n\n\nDemo Day: Moving policy reform files from Tax-Brain to Tax-Cruncher\n\n\n\n\n\n\ndemo-days\n\n\n\nHow to move reforms between a tax-unit-level and society-wide model with the Compute.Studio API.\n\n\n\n\n\nMar 2, 2021\n\n\nMatt Jensen\n\n\n\n\n\n\n\n\n\n\n\n\nDemo Day: Contributing to PSL projects\n\n\n\n\n\n\ndemo-days\n\n\nPSL\n\n\ngit\n\n\ngithub\n\n\n\nHow to help software projects in the Policy Simulation Library.\n\n\n\n\n\nMar 2, 2021\n\n\nJason DeBacker\n\n\n\n\n\n\n\n\n\n\n\n\nDemo Day: Running the scf and microdf Python packages in Google Colab\n\n\n\n\n\n\ndemo-days\n\n\nmicrodf\n\n\nscf\n\n\n\nAnalyzing US wealth data in a web-based Python notebook.\n\n\n\n\n\nJan 29, 2021\n\n\nMax Ghenis\n\n\n\n\n\n\n\n\n\n\n\n\nDemo Day: The OG-USA macroeconomic model of U.S. fiscal policy\n\n\n\n\n\n\ndemo-days\n\n\nOG-USA\n\n\nTax-Calculator\n\n\nopen-source\n\n\npolicy-simulation-library\n\n\ncompute-studio\n\n\nus\n\n\n\nHow to model the macroeconomic effects of tax reform with a web app.\n\n\n\n\n\nJan 28, 2021\n\n\nRichard W. Evans\n\n\n\n\n\n\n\n\n\n\n\n\nDemo Day: Tax-Brain\n\n\n\n\n\n\ndemo-days\n\n\nindividual-income-tax\n\n\ntax-brain\n\n\nus\n\n\n\nComputing the impact of US tax reform with the Tax-Brain web-app.\n\n\n\n\n\nDec 23, 2020\n\n\nAnderson Frailey\n\n\n\n\n\n\n\n\n\n\n\n\n2020: A year in review\n\n\n\n\n\n\npsl\n\n\npsl-foundation\n\n\n\nHighlights from the Policy Simulation Library in 2020.\n\n\n\n\n\nDec 23, 2020\n\n\nJason DeBacker\n\n\n\n\n\n\n\n\n\n\n\n\nDemo Day: Cost-of-Capital-Calculator Web Application\n\n\n\n\n\n\ndemo-days\n\n\ncost-of-capital-calculator\n\n\nbusiness-taxation\n\n\ncorporate-income-tax\n\n\n\nComputing the impact of taxes on business investment incentives under alternative policy scenarios.\n\n\n\n\n\nDec 3, 2020\n\n\nJason DeBacker\n\n\n\n\n\n\n\n\n\n\n\n\nDemo Day: Tax-Cruncher\n\n\n\n\n\n\ndemo-days\n\n\nindividual-income-tax\n\n\ntax-cruncher\n\n\n\nHow to calculate a taxpayer’s liabilities under current law and under a policy reform.\n\n\n\n\n\nNov 23, 2020\n\n\nPeter Metz\n\n\n\n\n\n\n\n\n\n\n\n\nDemo Day: Building policy reform files\n\n\n\n\n\n\ndemo-days\n\n\n\nThe first in Policy Simulation Library’s new live demo series describes specifying tax reforms.\n\n\n\n\n\nNov 18, 2020\n\n\nMatt Jensen\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing the PSL Blog\n\n\n\n\n\n\nannouncements\n\n\n\nA new way to follow models in the Policy Simulation Library catalog.\n\n\n\n\n\nNov 6, 2020\n\n\nMax Ghenis\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-03-03-DC-workshop.html",
    "href": "posts/2022-03-03-DC-workshop.html",
    "title": "Policy Simulation Library DC Workshop: Open source tools for analyzing tax policy",
    "section": "",
    "text": "The Policy Simulation Library is hosting a workshop in Washington, DC on March 25 on open source tools for the analysis of tax policy. Participants will learn how to use open source models from the Library for revenue estimation, distributional analysis, and to simulate economic impacts of tax policy. The workshop is intended to be a hands-on experience and participants can expect to leave with the required software, documentation, and knowledge to continue using these tools. All models in the workshop are written in the Python programming language–familiarity with the language is helpful, but not required.\nWorkshop Schedule:\n\n8:15-8:45a: Breakfast\n8:45-9:00a: Introduction\n9:00-9:50a: Using Tax-Calculator for revenue estimation and distributional analysis (Matt Jensen)\n10:00-10:50a: Estimating effective tax rates on investment with Cost-of-Capital-Calculator (Jason DeBacker)\n11:00-11:50a: Macroeconomic modeling of fiscal policy with OG-Core and OG-USA (Richard W. Evans)\nnoon-1:00p: Lunch and demonstration of PolicyEngine (Max Ghenis)\n\nThe workshop will be held at the Martin Luther King Jr. Memorial Library in Washington, DC. Participants are expected to arrive by 8:30am and the program will conclude at 1:00pm. Breakfast and lunch will be provided. PSL Foundation is sponsoring the event and there is no cost to attend. Attendance is limited to 30 in order to make this a dynamic and interactive workshop.\nTo register, please use this Google Form. Registration will close March 11. Participants will be expected to bring a laptop to the workshop where they can interact with the software in real time with the instructors. Registered participants will receive an email before the event with a list of software to install before the workshop.\nPlease feel free to share this invitation with your colleagues.\nQuestions about the workshop can be directed to Jason DeBacker at jason.debacker@gmail.com."
  },
  {
    "objectID": "posts/2020-12-23-demo-day-tax-brain.html",
    "href": "posts/2020-12-23-demo-day-tax-brain.html",
    "title": "Demo Day: Tax-Brain",
    "section": "",
    "text": "For this PSL demo-day I showed how to use the Tax-Brain web-application, hosted on Compute Studio, to analyze proposed individual income tax policies. Tax-Brain integrates the Tax-Calculator and Behavioral-Responses models to make running both static and dynamic analyses of the US federal income and payroll taxes simple. The web interface for the model makes it possible for anyone to run their own analyses without writing a single line of code.\nWe started the demo by simply walking through the interface and features of the web-app before creating our own sample reform to model. This reform, which to my knowledge does not reflect any proposals currently up for debate, included changes to the income and payroll tax rates, bringing back personal exemptions, modifying the standard deduction, and implementing a universal basic income.\nWhile the model ran, I explained how Tax-Brain validated all of the user inputs, the data behind the model, and how the final tax liability projections are determined. We concluded by looking through the variety of tables and graphs Tax-Brain produces and how they can easily be shared with others.\nResources:\n\nSimulation from the demonstration\nTax-Brain GitHub repo\nTax-Calculator documentation\nBehavioral-Responses documentation"
  },
  {
    "objectID": "posts/2021-11-01-demo-day-og-core.html",
    "href": "posts/2021-11-01-demo-day-og-core.html",
    "title": "Demo Day: The OG-Core platform",
    "section": "",
    "text": "The OG-Core model is a general equilibrium, overlapping generations (OG) model suitable for evaluating fiscal policy. Since the work of Alan Auerbach and Laurence Kotlikoff in the 1980s, this class of model has become a standard in the macroeconomic analysis of tax and spending policy. This is for good reason. OG models are able to capture the impacts of taxes and spending in the short and long run, examine incidence of policy across generations of people (not just short run or steady state analysis of a cross-section of the economy), and capture important economic dynamics (e.g., crowding out effects of deficit-financed policy).\nIn the PSL Demo Day presentation linked above, I cover the basics of OG-Core: its history, its API, and how country-specific models can use OG-Core as a dependency. In brief, OG-Core provides a general overlapping generations framework, from which parameters can be calibrated to represent particular economies. Think of it this way: an economic model is just a set of parameters plus a system of equations. OG-Core spells out all of the equations to represent an economy with heterogeneous agents, production and government sectors, open economy options, and detailed policy rules. OG-Core also includes default values for all parameters, along with parameter metadata and parameter validation rules. A country specific application is then just a particular parameterization of the general OG-Core model.\nAs an example of a country-specific application, one can look at the OG-USA model. This model provides a calibration of OG-Core to the United States. The source code in that project allows one to go from raw data sources to the estimation and calibration procedures used to determine parameter values representing the United States, to parameter values in formats suitable for use in OG-Core. Country-specific models like OG-USA include (where available) links to microsimulation models of tax and spending programs to allow detailed microdata of actual and counterfactual policies to inform the net tax-transfer functions used in the OG-Core model. For those interested in building their own country-specific model, the OG-USA project provides a good example to work from.\nWe encourage you to take a look at OG-Core and related projects. New contributions and applications are always welcome. If you have questions or comments, reach out through the relevant repositories on Github to me, @jdebacker, or Rick Evans, @rickecon.\nResources:\n\nOG-Core documentation\nOG-USA documentation package for unit testing in Python\nTax-Calculator documentation\nOG-UK repository\nOpenFisca-UK repository\nSlides from the Demo Day presentation"
  },
  {
    "objectID": "posts/2020-12-03-demo-day-cost-of-capital-calculator.html",
    "href": "posts/2020-12-03-demo-day-cost-of-capital-calculator.html",
    "title": "Demo Day: Cost-of-Capital-Calculator Web Application",
    "section": "",
    "text": "In the PSL Demo Day video linked above, I demonstrate how to use the Cost-of-Capital-Calculator (CCC) web application on Compute-Studio. CCC computes various measures of the impact of the tax system on business investment. These include the Hall-Jorgenson cost of capital, marginal effective tax rates, and effective average tax rates (following the methodology of Devereux and Griffith (1999)).\nI begin by illustrating the various parameters available for the user to manipulate. These include parameters of the business and individual income tax systems, as well as parameters representing economic assumptions (e.g., inflation rates and nominal interest rates) and parameters dictating financial and accounting policy (e.g., the fraction of financing using debt). Note that all default values for tax policy parameters represent the “baseline policy”, which is defined as the current law policy in the year being analyzed (which itself is a parameter the user can change). Other parameters are estimated using historical data following the methodology of CBO (2014).\nNext, I change a few parameters and run the model. In this example, I move the corporate income tax rate up to 28% and lower bonus depreciation for assets with depreciable lives of 20 years or less to 50%.\nFinally, I discuss how to interpret output. The web app returns a table and three figures summarizing marginal effective total tax rates on new investments. This selection of output helps give one a sense of the the overall changes, as well as effects across asset types, industries, and type of financing. For the full model output, one can click on “Download Results”. Doing so will download four CSV files contain several measures of the impact of the tax system on investment for very fine asset and industry categories. Users can take these files and create tables and visualizations relevant to their own use case.\nPlease take the model for a spin and simulate your own reform. If you have questions, comments, or suggestions, please let me know on the PSL Discourse (non-technical questions) or by opening an issue in the CCC GitHub repository (technical questions).\nResources:\n\nCompute Studio simulation used in the demonstration\nCost-of-Capital-Calculator web app\nCost-of-Capital-Calculator documentation\nCost-of-Capital-Calculator GitHub repository"
  },
  {
    "objectID": "posts/2020-11-23-demo-day-tax-cruncher.html",
    "href": "posts/2020-11-23-demo-day-tax-cruncher.html",
    "title": "Demo Day: Tax-Cruncher",
    "section": "",
    "text": "For the Demo Day on November 16, I showed how to calculate a taxpayer’s liabilities under current law and under a policy reform with Tax-Cruncher. The Tax-Cruncher web application takes two sets of inputs: a taxpayer’s demographic and financial information and the provisions of a tax reform.\nFor the first Demo Day example (3:50), we looked at how eliminating the state and local tax (SALT) deduction cap and applying payroll tax to earnings above $400,000 would affect a high earner. In particular, our hypothetical filer had $500,000 in wages, $100,000 in capital gains, and $100,000 in itemizable expenses. You can see the results at Compute Studio simulation #634.\nFor the second example (17:50), we looked at how expanding the Earned Income Tax Credit (EITC) and Child Tax Credit would impact a family with $45,000 in wages and two young children. You can see the results at Compute Studio simulation #636.\nResources:\n\nTax-Cruncher\nTax-Cruncher-Biden"
  },
  {
    "objectID": "posts/2021-01-28-demo-day-how-to-use-og-usa.html",
    "href": "posts/2021-01-28-demo-day-how-to-use-og-usa.html",
    "title": "Demo Day: The OG-USA macroeconomic model of U.S. fiscal policy",
    "section": "",
    "text": "In this PSL Demo Day, I demonstrate how to use the open source OG-USA macroeconomic model of U.S. fiscal policy. Jason DeBacker and I (Richard Evans) have been the core maintainers of this project and repository for the last six years. This demo is organized into the following sections. The YouTube webinar linked above took place on January 11, 2021."
  },
  {
    "objectID": "posts/2021-01-28-demo-day-how-to-use-og-usa.html#brief-note-about-the-value-of-the-psl-community",
    "href": "posts/2021-01-28-demo-day-how-to-use-og-usa.html#brief-note-about-the-value-of-the-psl-community",
    "title": "Demo Day: The OG-USA macroeconomic model of U.S. fiscal policy",
    "section": "Brief note about the value of the PSL community",
    "text": "Brief note about the value of the PSL community\nThe Policy Simulation Library is a decentralized organization of open source policy models. The Policy Simulation Library GitHub organization houses many open source repositories, each of which represents a curated policy project by a diverse group of maintainers. The projects that have met the highest standards of best practices and documentation are designated as psl-cataloged , while newer projects that are in earlier stages are designated as psl-incubating . The philosophy and goal of the PSL environment is to make policy modeling open and transparent. It also allows more collaboration and cross-project contributions and interactions.\nThe Policy Simulation Library group has been holding these PSL Demo Day webinars since the end of 2020. The video of each webinar is available on the Policy Simulation Library YouTube channel. These videos are a great resource for learning the different models available in the PSL community, how the models interact, how to contribute to them, and what is on the horizon in their development. Also excellent in many of the PSL Demo Day webinars is a demonstration of how to use the models on the Compute Studio web application platform.\nI have been a participant in and contributor to the PSL community since its inception. I love economic policy modeling. And I learned how sophisticated and complicated economic policy models can be. And any simulation can have hundreds of underlying assumptions, some of which may not be explicitly transparent. I think models that are used for public policy analysis have a philosophical imperative to be open source. This allows others to verify results and test sensitivity to assumptions.\nAnother strong benefit of open source modeling is that it is fundamentally apolitical. With proprietary closed-source policy models, an outside observer might criticize the results of the model based on the perceived political biases of the modeler or the sponsoring organization. With open-source models, a critic can be redirected to the underlying assumptions, structure, and content of the model. This is constructive criticism and debate that moves the science foreward. In the current polarized political environment in the U.S., open-source modeling can provide a constructive route for bipartisan cooperation and the democratization of computational modeling. Furthermore, open-source modeling and workflow encourages the widest forms of collaboration and contributions."
  },
  {
    "objectID": "posts/2021-01-28-demo-day-how-to-use-og-usa.html#description-of-og-usa-model",
    "href": "posts/2021-01-28-demo-day-how-to-use-og-usa.html#description-of-og-usa-model",
    "title": "Demo Day: The OG-USA macroeconomic model of U.S. fiscal policy",
    "section": "Description of OG-USA model",
    "text": "Description of OG-USA model\nOG-USA is an open-source overlapping generations, dynamic general equilibrium, heterogeneous agent, macroeconomic model of U.S. fiscal policy. The GitHub repository for the OG-USA source code is github.com/PSLmodels/OG-USA. This repository contains all the source code and instructions for loading and running OG-USA and all of its dependencies on your local machine. We will probably do another PSL Demo Day on how to run OG-USA locally. This Demo Day webinar is about running OG-USA on the Compute Studio web application. See Section “Using OG-USA on Compute.Studio” below.\nAs a heterogeneous agent macroeconomic model, OG-USA allows for distributional analyses at the individual and firm level. That is, you can simulate the model and answer questions like, “How will an increase in the top three personal income tax rates affect people of different ages and income levels?” Microsimulation models can answer these types of distributional analysis questions as well. However, the difference between a macroeconomic model and a microsimulation model is that the macroeconomic models can simulate how each of those individuals and firms will respond to a policy change (e.g., lower labor supply or increased investment demand) and how those behavioral responses will add up and feed back into the macroeconomy (e.g., the effect on GDP, government revenue, government debt, interest rates, and wages).\nOG-USA is a large-scale model and comprises tens of thousands of lines of code. The status of all of this code being publicly available on the internet with all collaboration and updates also public makes this an open source project. However, it is not enough to simply post one’s code. We have gone to great lengths to make in-line comments or “docstring” in the code to clarify the purpose of each function and line of code. For example, look in the OG-USA/ogusa/household.py module. The first function on line 18 is the marg_ut_cons() function. As is described in its docstring, its purpose is to “Compute the marginal utility of consumption.”\nThese in-code docstrings are not enough. We have also created textbook style OG-USA documentation at pslmodels.github.io/OG-USA/ using the Jupyter Book medium. This form of documentation has the advantage of being in book form and available online. It allows us to update the documentation in the open-source repository so changes and versions can be tracked. It describes the OG-USA API, OG-USA theory, and `OG-USA calibration. As with the model, this documentation is always a work in progress. But being open-source allows outside contributors to help with its updated and error checking.\nOne particular strength of the OG-USA model I want to highlight is its interaction with microsimulation models to incorporate information about tax incentives faced by the heterogeneous households in the model. We have interfaced OG-USA with microsimulation models in India and in the European Commission. OG-USA ’s default for modeling the United States is to use the open-source Tax-Calculator microsimulation model, which was described by Anderson Frailey in the last Demo Day of 2020. However, DeBacker and I currently have a project in which we use OG-USA to simulate policies using the Tax Policy Center’s microsimulation model. The way OG-USA interfaces with microsimulation models to incorporate rich tax data is described in the documentation in the calibration chapter entitled, “Tax Functions”."
  },
  {
    "objectID": "posts/2021-01-28-demo-day-how-to-use-og-usa.html#using-og-usa-on-compute-studio",
    "href": "posts/2021-01-28-demo-day-how-to-use-og-usa.html#using-og-usa-on-compute-studio",
    "title": "Demo Day: The OG-USA macroeconomic model of U.S. fiscal policy",
    "section": "Using OG-USA on Compute Studio",
    "text": "Using OG-USA on Compute Studio\nIn the demonstration, I focus on how to run experiments and simulations with OG-USA using the Compute Studio web application platform rather than installing running the model on your local machine. To use OG-USA on this web application, you will need a Compute Studio account. Once you have an account, you can start running any model available through the site. For some models, you will have to pay for the compute time, although the cost of running these models is very modest. However, all Compute Studio simulations of the OG-USA model are currently sponsored by the Open Source Economics Laboratory. This subsidy will probably run out in the next year. But we are always looking for funding for these models.\nOnce you are signed up and logged in to your Compute Studio account, you can go to the OG-USA model on Compute Studio at compute.studio/PSLmodels/OG-USA. The experiment that we simulated in the demonstration is available at compute.studio/PSLmodels/OG-USA/206. The description at the top of the simulation page describes the changes we made. You can look through the input page by clicking on the “Inputs” tab. We ran the model by clicking the green “Run” button at the lower left of the page. The model took about 5 hours to run, so I pre-computed the results that we discussed in the demo. The outputs of the experiment are available in the “Outputs” tab on the page. I also demonstrated how one can click the “Download Results” button at the bottom of the “Outputs” tab to download more results from the simulation. However, the full set of results is only available by installing and running the OG-USA model simulation on your local machine.\nThe benefits of the Compute Studio web application are that running the OG-USA model is much easier for the non-expert, and the multiple-hour computation time can be completed on a remote machine in the cloud."
  },
  {
    "objectID": "posts/2021-01-28-demo-day-how-to-use-og-usa.html#resources",
    "href": "posts/2021-01-28-demo-day-how-to-use-og-usa.html#resources",
    "title": "Demo Day: The OG-USA macroeconomic model of U.S. fiscal policy",
    "section": "Resources",
    "text": "Resources\n\nPSL Demo Day YouTube webinar: “How to use OG-USA”\nOG-USA on Compute Studio\nSimulation from the demonstration\nOG-USA GitHub repo\nOG-USA documentation\nTax-Calculator GitHub repo"
  },
  {
    "objectID": "posts/2021-12-08-demo-day-synthimpute.html",
    "href": "posts/2021-12-08-demo-day-synthimpute.html",
    "title": "Demo Day: Using synthimpute for data fusion",
    "section": "",
    "text": "Suppose a policy analyst sought to estimate the impact of a policy that changed income tax rates and benefit rules while also adding a progressive wealth tax. The standard approach is to use a microsimulation model, where the rules are programmed as code, and then to run that program over a representative sample of households. Unfortunately, no single US government survey captures all the households characteristics needed to analyze this policy; in particular, the reliable tax and benefit information lies in surveys like the Current Population Survey (CPS), while wealth lies in the Survey of Consumer Finances (SCF).\nAssuming the analyst wanted to start with the CPS, they have several options to estimate wealth for households to levy the progressive wealth tax. Two typical approaches include:\n\nLinear regression, predicting wealth from other household characteristics common to the CPS and SCF.\nMatching, in which each CPS household is matched with the most similar household in the SCF.\n\nNeither of these approaches, however, aim to estimate the distribution of wealth conditional on other characteristics. Linear regression explicitly estimates the mean prediction, but that could miss the tails of wealth from whom most of the wealth tax revenue will be collected.\nInstead, the analyst could apply quantile regression to estimate the distribution of wealth conditional on other characteristics, and then measure the effectiveness of the estimation using quantile loss.\nIn this Demo Day, I present the concepts of microsimulation, imputation, and quantile loss to motivate the synthimpute Python package I’ve developed with my PolicyEngine colleague Nikhil Woodruff. In an experiment predicting wealth on a holdout set from the SCF, my former colleague Deepak Singh and I found that random forests significantly outperform OLS and matching for quantile regression, and this is the approach applied in synthimpute for both data fusion and data synthesis. The synthimpute API will be familiar to users of scikit-learn and statsmodels , with the difference being that synthimpute ’s rf_impute function returns a random value from the predicted distribution; it can also skew the predictions to meet a target total.\nWe’ve used synthimpute to fuse data for research reports at the UBI Center and to enhance the PolicyEngine web app for UK tax and benefit simulation, and we welcome new users and contributors. Feel free to explore the repository or contact me with questions at max@policyengine.org.\nResources:\n\nsynthimpute package on GitHub\nPresentation slides\nUBI Center report on land value taxation in the UK, using synthimpute to impute land value from the UK Wealth and Assets Survey to the Family Resources Survey\nPolicyEngine UK carbon tax example, using synthimpute to impute carbon emissions from the Living Costs and Food Survey to the Family Resources Survey\nNotebook comparing random forests to matching and other techniques using a holdout set from the US Survey of Consumer Finances\nMy blog post on quantile regression for Towards Data Science, which laid the groundwork for synthimpute"
  },
  {
    "objectID": "posts/2021-04-05-demo-day-stacked-revenue-estimates.html",
    "href": "posts/2021-04-05-demo-day-stacked-revenue-estimates.html",
    "title": "Demo Day: Producing stacked revenue estimates with the Tax-Calculator Python API",
    "section": "",
    "text": "It’s often useful to be able to identify the effects of specific provisions individually and not just the overall impact of a proposal with many provisions. Indeed, when revenue estimates of tax law changes are reported (such as this JCT analysis of the American Rescue Plan Act of 2021), they are typically reported on a provision-by-provision basis. Finding the provision-by-provision revenue estimates is cumbersome with the Tax-Brain web application both because it’s hard to iterate over many provisions and because the order matters when stacking estimates, so that one needs to keep this order in mind as parameter values are updated for each additional provision in a full proposal.\nIn the PSL Demo Day on April 5, 2021, I show how to use the Python API of Tax-Calculator to efficiently produce stacked revenue estimates. In fact, after some initial setup, this can be done with just 12 lines of code (plus a few more to make the output look nice). The Google Colab notebook that illustrates this approach can be found at this link, but here I’ll walk through the four steps that are involved:\n\nDivide up the full proposal into strings of JSON text that contain each provision you want to analyze. My example breaks up the Biden 2020 campaign proposal into seven provisions, but this is illustrative and you can make more or less provisions depending on the detail you would like to see.\nCreate a dictionary that contains, as its values, the JSON strings. A couple notes on this. First, the dictionary keys should be descriptive of the provisions as they will become the labels for each provision in the final table of revenue estimates we produce. Second, order matters here. You’ll want to be sure the current law baseline is first (the value for this will be an empty dictionary). Then you specify the provisions. The order you specify will likely affect your revenue estimates from a given provision (for instance, expanding/restricting a deduction has a larger revenue effect when rates are higher), but there are not hard and fast rules on the “right” order. Traditionally, rate changes are stacked first and tax expenditures later in the order.\nIterate over this dictionary. With a dictionary of provisions in hand, we can write a “for loop” to iterate over the provision, simulating the Tax-Calculator model at each step. Note that when the Policy class object in Tax-Calculator is modified, it only needs to be told the changes in tax law parameters relative to its current state. In other words, when we are stacking provisions, estimating the incremental effect of each, you can think of the Policy object having a baseline policy that is represented by the current law baseline plus all provisions that have been analyzed before the provision at the current iteration. The Policy class was created in this way so that one can easily represent policy changes, requiring the user to only input the set of parameters that are modified, not every single parameter’s value under the hypothetical policy. But this also makes it parsimonious to stack provisions as we are doing here. Notice that the JSON strings for each provision (created in Step 1) can be specified independent of the stacking order. We only needed to slice the full set of proposals into discrete chunks, we didn’t need to worry about creating specifications of cumulative policy changes.\nFormat output for presentation. After we’ve run a Tax-Calculator simulation for the current law baseline plus each provision (and each year in the budget window), we’ve got all the output we need. With this output, we can quickly create a table that will nicely present our stacked revenue estimate. One good check to do here is to create totals across all provisions and compare this to the simulated revenue effects of running the full set of proposals in one go. This check helps to ensure that you didn’t make an error in specifying your JSON strings. For example, it’s easy to leave out one or more provisions, especially if there are many.\n\nI hope this provides a helpful template for your own analysis. Note that one can modify this code in several useful ways. For example, within the for-loops, the Behavioral-Responses can be called to produce revenue estimates that take into account behavioral feedback. Or one could store the individual income tax and payroll tax revenue impacts separately (rather than return the combined values as in the example notebook). Additional outputs (even the full set of microdata after each provision is applied) can be stored for even more analysis.\nIn the future, look for Tax-Brain to add stacked revenue estimates to its capabilities. It’ll still be important for users to carve up their full list of policy changes into sets of provisions as we did in Steps 1 and 2 above, but Tax-Brain will then take care of the rest behind the scenes.\nResources:\n\nColab Notebook with example\nBiden campaign reform file in PSL Examples"
  },
  {
    "objectID": "posts/2020-12-23-2020-year-in-review.html",
    "href": "posts/2020-12-23-2020-year-in-review.html",
    "title": "2020: A year in review",
    "section": "",
    "text": "This year has been one to forget! But 2020 did have its bright spots, especially in the PSL community. This post reviews some of the highlights from the year.\nThe Library was able to welcome two new models to the catalog in 2020: microdf and OpenFisca-UK. microdf provides a number of useful tools for use with economic survey data. OpenFisca-UK builds off the OpenFisca platform, offering a microsimulation model for tax and benefit programs in the UK.\nIn addition, four new models were added to the Library as incubating projects. The ui-calculator model has received a lot of attention this year in the U.S., as it provides the capability to calculate unemployment insurance benefits across U.S. states, a major mode of delivering financial relief to individuals during the COVID crisis. PCI-Outbreak directly relates to the COVID crisis, using machine learning and natural language processing to estimate the true extent of the COVID pandemic in China. The model finds that actual COVID cases are significantly higher than what official statistics claim. The COVID-MCS model considers COVID case counts and test positivity rates to measure whether or not U.S. communities are meeting certain benchmarks in controlling the spread of the disease. On a lighter note, the Git-Tutorial project provides instruction and resources for learning to use Git and GitHub, with an emphasis on the workflow used by many projects in the PSL community.\nThe organization surrounding the Policy Simulation Library has been bolstered in two ways. First, we have formed a relationship with the Open Collective Foundation, who is now our fiscal host. This allows PSL to accept tax deductible contributions that will support the efforts of the community. Second, we’ve formed the PSL Foundation, with an initial board that includes Linda Gibbs, Glenn Hubbard, and Jason DeBacker.\nOur outreach efforts have grown in 2020 to include the regular PSL Demo Day series and this PSL Blog. Community members have also presented work with PSL models at the PyData Global Conference, the Tax Economists Forum, AEI, the Coiled Podcast, and the Virtual Global Village Podcast. New users will also find a better experience learning how to use and contribute to PSL models as many PSL models have improved their documentation through the use of Jupyter Book (e.g., see the Tax-Calculator documentation).\nWe love seeing the community around open source policymaking expand and are proud that PSL models have been used for important policy analysis in 2020, including analyzing economic policy responses to the pandemic and the platforms of presidential candidates. We look forward to more progress in 2021 and welcome you to join the effort as a contributor, financially or as an open source developer.\nBest wishes from PSL for a happy and healthy new year!\nResources:\n\nPSL Twitter Feed\nPSL YouTube\nPSL on Open Collective"
  },
  {
    "objectID": "posts/2021-05-17-demo-day-jupyter-book-deploy.html",
    "href": "posts/2021-05-17-demo-day-jupyter-book-deploy.html",
    "title": "Demo Day: Updating Jupyter Book documentation with GitHub Actions",
    "section": "",
    "text": "Open source projects must maintain clear and up-to-date documentation in order to attract users and contributors. Because of this, PSL sets minimum standards for documentation among cataloged projects in its model criteria. A recent innovation in executable books, Jupyter Book, has provided an excellent format for model documentation and has been widely adopted by PSL projects (see for example OG-USA, Tax-Brain, Tax-Calculator).\nJupyter Book allows one to write documents with executable code and text together, as in Jupyter notebooks. But Jupyter Book pushes this further by allowing documents with multiple sections, better integration of TeX for symbols and equations, BibTex style references and citations, links between sections, and Sphinx integration (for auto-built documentation of model APIs from source code). Importantly for sharing documentation, Jupyter Books can easily be compiled to HTML, PDF, or other formats. Portions of a Jupyter Book that contain executable code can be downloaded as Jupyter Notebooks or opened in Google Colab or binder\nThe Jupyter Book documentation is excellent and will help you get started creating your “book” (tip: pay close attention to formatting details, including proper whitespace). What I do here is outline how you can easily deploy your documentation to the web and keep it up-to-date with your project.\nI start from the assumption that you have the source files to build your Jupyter Book checked into the main branch of your project (these maybe yml , md , rst , ipynb or other files). For version control purposes and to keep your repo trim, you generally don’t want to check the built documentation files to this branch (tip: consider adding the folder these files will go to (e.g., /_build to your .gitignore ). When these files are in place and you can successfully build your book locally, it’s time for the first step.\nStep 1: Add two GH Actions to your project’s workflow: 1. An action to check that your documentation files build without an error. I like to run this on each push to a PR. The action won’t hang on warnings, but will fail if your Jupyter Book doesn’t build at all. An example of this action from the OG-USA repo is here:\n\nname: Check that docs build\non: [push, pull_request]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2 # If you're using actions/checkout@v2 you must set persist-credentials to false in most cases for the deployment to work correctly.\n        with:\n          persist-credentials: false\n\n      - name: Setup Miniconda\n        uses: conda-incubator/setup-miniconda@v2\n        with:\n          activate-environment: ogusa-dev\n          environment-file: environment.yml\n          python-version: 3.7\n          auto-activate-base: false\n\n      - name: Build # Build Jupyter Book\n        shell: bash -l {0}\n        run: |\n          pip install jupyter-book\n          pip install sphinxcontrib-bibtex==1.0.0\n          pip install -e .\n          cd docs\n          jb build ./book\nTo use this in your repo, you’ll just need to change a few settings such as the name of the environment and perhaps the Python version and path to the book source files. Note that in the above yml file sphinxcontrib-bibtex is pinned. You maybe able to unpin this, but OG-USA needed this pin for documentation to compile property due to changes in the jupyter-book and sphinxcontrib-bibtex packages.\n\nAn action that builds and deploys the Jupyter Book to GH Pages. The OG-USA project uses the deploy action from James Ives in this action. This is something that you will want to run when PRs are merged into your main branch so that the documentation is kept up-to-date with the project. To modify this action for your repo, you’ll need to change the repo name, the environment name, and potentially the Python version, branch name, and path to the book source files.\n\nStep 2: Once the action in (2) above is run, your compiled Jupyter Book docs will be pushed to a gh-pages branch in your repository (the action will create this branch for you if it doesn’t already exist). At this point, you should be able to see your docs at the url https://GH_org_name.github.io/Repo_name . But it probably won’t look very good until you complete this next step. To have your Jupyter Book render on the web as you see it on your machine, you will want to push and merge an empty file with the name .nojekyll into your repo’s gh-pages branch.\nThat’s it! With these actions, you’ll be sure that your book continues to compile and a new version will be published to the web with with each merge to your main branch, ensuring that your documentation stays up-to-date.\nSome additional tips:\n\nUse Sphinx to document your projects API. By doing so you’ll automate an important part of your project’s documentation – as long as the docstrings are updated when the source code is, the Jupyter Book you are publishing to the web will be kept in sync with no additional work needed.\nYou can have your gh-pages-hosted documentation point to a custom URL.\nProject maintainers should ensure that docs are updated with PRs that are relevant (e.g., if the PR changes an the source code affecting a user interface, then documentation showing example usage should be updated) and help contributors make the necessary changes to the documentation source files."
  },
  {
    "objectID": "posts/2021-07-16-demo-day-constructing-tax-data-for-the-50-states.html",
    "href": "posts/2021-07-16-demo-day-constructing-tax-data-for-the-50-states.html",
    "title": "Demo Day: Constructing tax data for the 50 states",
    "section": "",
    "text": "Federal income tax reform impacts can vary dramatically across states. The cap on state and local tax deductions (SALT) is a well-known example, but other policies also have differential effects because important tax-relevant features vary across states such as the income distribution, relative importance of wage, business, and retirement income, and family size and structure. Analyzing how policy impacts vary across states requires data that faithfully represent the characteristics of the 50 states.\nThis Demo Day described a method and software for constructing state weights for microdata files that (1) come as close as possible to targets for individual states, while (2) ensuring that the state weights for each tax record sum to its national weight. The latter objective ensures that the sum of state impacts for a tax reform equals the national impact.\nThis project developed state weights for a data file with more than 200,000 microdata records. The weighted data file comes within 0.01% of desired values for more than 95% of approximately 10,000 targets.\nThe goal of the slides and video was to enable a motivated Python-skilled user of the PSL TaxData and Tax-Calculator projects to reproduce project results: 50-state weights for TaxData’s primary output, the puf.csv microdata file (based primarily on an IRS Public Use File), using early-stage open-source software developed in the project. Thus, the demo is technical and focused on nuts and bolts.\nThe methods and software can also be used to:\n\nCreate geographic-area weights for other microdata files\nApportion state weights to Congressional Districts or counties, if suitable targets can be developed\nCreate state-specific microdata files suitable for modeling state income taxes\n\nThe main topics covered in the slides and video are:\n\nCreating national and state targets from IRS summary data\nPreparing a national microdata file for state weighting\nApproaches to constructing geographic weights\nRunning software that implements the Poisson-modeling approach used in the project\nMeasures of quality of the results"
  },
  {
    "objectID": "posts/2021-01-29-demo-day-scf-microdf.html",
    "href": "posts/2021-01-29-demo-day-scf-microdf.html",
    "title": "Demo Day: Running the scf and microdf Python packages in Google Colab",
    "section": "",
    "text": "For Monday’s PSL Demo Day, I showed how to use the scf and microdf PSL Python packages from the Google Colab web-based Jupyter notebook interface.\nThe scf package extracts data from the Federal Reserve’s Survey of Consumer Finances, the canonical source of US wealth microdata. scf has a single function: load(years, columns) , which then returns a pandas DataFrame with the specified column(s), each record’s survey weight, and the year (when multiple years are requested).\nThe microdf package analyzes survey microdata, such as that returned by the scf.load function. It offers a consistent paradigm for calculating statistics like means, medians, sums, and inequality statistics like the Gini index. Most functions are structured as follows: f(df, col, w, groupby) where df is a pandas DataFrame of survey microdata, col is a column(s) name to be summarized, w is the weight column, and groupby is the column(s) to group records in before summarizing.\nUsing Google Colab, I showed how to use these packages to quickly calculate mean, median, and total wealth from the SCF data, without having to install any software or leave the browser. I also demonstrated how to use the groupby argument of microdf functions to show how different measures of wealth inequality have changed over time. Finally, I previewed some of what’s to come with scf and microdf : imputations, extrapolations, inflation, visualization, and documentation, to name a few priorities.\nResources:\n\nSlides\nDemo notebook in Google Colab\nSimulation from the demonstration\nscf GitHub repo\nmicrodf GitHub repo\nmicrodf documentation"
  },
  {
    "objectID": "posts/2021-08-09-demo-day-unit-testing.html",
    "href": "posts/2021-08-09-demo-day-unit-testing.html",
    "title": "Demo Day: Unit testing for open source projects",
    "section": "",
    "text": "Unit testing is the testing of individual units or functions of a software application. This differs from regression testing that focuses on the verification of final outputs. Instead, unit testing tests each smallest testable component of your code. This helps to more easily identify and trace errors in the code.\nWriting unit tests is good practice, though not one that’s always followed. The biggest barrier to writing unit tests is that doing so takes time. You might wonder “why am I testing code that runs?” But there are a number benefits to writing unit tests:\n\nIt ensures that the code does what you expect it to do\nYou’ll better understand what your code is doing\nYou will reduce time tracking down bugs in your code\n\nOften, writing unit tests will save you time in the longer run because it reduces debugging time and because it forces you to think more about what your code does, which often leads to the development of more efficient code. And for open source projects, or projects with many contributors, writing unit tests is important in reducing the likelihood that errors are introduced into your code. This is why the PSL catalog criteria requires projects to provide at least some level of unit testing.\nIn the PSL Demo Day video linked above, I illustrate how to implement unit tests in R using the testthat package. There are essentially three steps to this process:\n\nCreate a directory to put your testing script in, e.g., a folder called tests\nCreate one or more scripts that define your tests.\n\nEach test is represented as a call of the test_that function and contain an statement that will evaluate as true or false (e.g., you may use the expect_equal function to verify that a function returns expected values given certain inputs).\nYou will want to use test in the name of these tests scripts as well as something descriptive of what is tested.\n\nCreate a script that will run your tests.\n\nHere you’ll need to import the testthat package and you’ll need to call the script(s) you are testing to load their functions.\nThen you’ll use the test_dir function to pass the directory in which the script(s) you created in Step 2 reside.\n\n\nCheck out the video to see examples of how each of these steps is executed. I’ve also found this blog post on unit tests with testthat to be helpful.\nUnit testing in Python seems to be more developed and straightforward with the excellent pytest package. While pytest offers many options for parameterizing tests, running tests in parallel, and more, the basic steps remain the same as those outlined above:\n\nCreate a directory for your test modules (call this folder tests as pytest will look for that name).\nCreate test modules that define each test\n\nTests are defined much like any other function in Python, the but will involve some assertion statement is triggered upon test failure.\nYou will want to use test in the name of these tests modules as well as something descriptive of what is tested.\n\nYou won’t need to create a script to run your tests as with testthat, but you may create a pytest.ini file to customize your tests options.\n\nThat’s about it to get started writing unit tests for your code. PSL cataloged projects provide many excellent examples of a variety of unit tests, so search them for examples to build from. In a future Demo Day and blog post, we’ll talk about continuous integration testing to help get even more out of you unit tests.\nResources:\n\ntestthat package for unit testing in R\npytest package for unit testing in Python\nPSL catalog criteria\nUnit tests for the capital-cost-recovery model"
  }
]